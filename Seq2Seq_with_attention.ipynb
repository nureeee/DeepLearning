{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq with attention",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nureeee/DeepLearning/blob/main/Seq2Seq_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G94VfLE_3D6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35943b4f-c5d8-40f2-d89a-6d25ef32c472"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 58.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FgS9Cyg3O8M"
      },
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "from konlpy.tag import Okt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNoK05fg3ct1"
      },
      "source": [
        "## 하이퍼 파라미터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKHTS9VS3sCK"
      },
      "source": [
        "num_epochs=200\n",
        "vocab_size=2000"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9AZlQjG32Wn"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhBMlIs03_Kt"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.emb  = tf.keras.layers.Embedding(vocab_size, 64)\n",
        "\n",
        "    self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)\n",
        "\n",
        "  def call(self, x, training=False):\n",
        "    x = self.emb(x)\n",
        "\n",
        "    hidden_state, last_hidden_state, last_cell_state = self.lstm(x)\n",
        "\n",
        "    return hidden_state, last_hidden_state, last_cell_state"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfSgMxS54BcE"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saQMNwmd4Ccy"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.emb = tf.keras.layers.Embedding(vocab_size, 64)\n",
        "    self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)\n",
        "\n",
        "    self.att = tf.keras.layers.Attention()\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    x, s0, c0, H = inputs\n",
        "\n",
        "    x = self.emb(x) \n",
        "    S, h, c = self.lstm(x, initial_state=[s0, c0])\n",
        "    \n",
        "    S_ = tf.concat([s0[:, tf.newaxis, :], S[:,:-1,:]], axis=1)\n",
        "\n",
        "    A = self.att([S_, H])\n",
        "    \n",
        "    V = tf.concat([S, A], axis=-1)\n",
        "    y = self.dense(V)\n",
        "\n",
        "    return y, h, c"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEdVpeTZ4ENi"
      },
      "source": [
        "# Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZUxAvrd4LyT"
      },
      "source": [
        "class Seq2seq(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self, sos, eos):\n",
        "    super(Seq2seq, self).__init__()\n",
        "    self.sos = sos # decoder에서 사용되어질 sos\n",
        "    self.eos = eos # encoder에서 사용되어질 eos\n",
        "\n",
        "    self.enc = Encoder()\n",
        "    self.dec = Decoder()\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    \n",
        "    if training:\n",
        "\n",
        "      x, y = inputs\n",
        "      H, h,c = self.enc(x)       \n",
        "      y, _, __ = self.dec((y, h, c, H)) # teacher forcing. Decoder의 입력으로 Shifted Output을 넣어줌\n",
        "      \n",
        "      return y\n",
        "    else: # 테스트 할 때는 x만 들어 온다..\n",
        "      x = inputs\n",
        "      H, h, c = self.enc(x) # last_cell_state, last_hidden_state\n",
        "      \n",
        "      # <sos> 입력\n",
        "      # <sos> 토큰을 tensor 배열화 시켜야 함\n",
        "      y = tf.convert_to_tensor(self.sos) # 0 rank tensor로 변환\n",
        "      y = tf.reshape(y, (1, 1)) # <sos>가 (1, 1)형식으로 변환. ( 1배치, 1타임 스텝)을 의미. embedding 레이어에 넣을 예정\n",
        "\n",
        "      seq = tf.TensorArray(tf.int32, 64) # 64개의 텐서 배열 만들어 놓기\n",
        "\n",
        "      for idx in tf.range(64):\n",
        "        y, h, c = self.dec([y, h, c, H]) # 리턴 받는 y는 softmax의 결과\n",
        "        y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32)\n",
        "\n",
        "        y = tf.reshape(y, (1, 1)) # (1 배치, 1단어를 의미하기 위해 reshape - 테스트 할 때는 배치를 1로 설정할 예정..)\n",
        "\n",
        "        seq = seq.write(idx, y) # 순서대로 write\n",
        "\n",
        "        if y == self.eos:\n",
        "          break\n",
        "      \n",
        "      return tf.reshape(seq.stack(), (1, 64))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWvpZtLKXEKL"
      },
      "source": [
        "# 학습, 테스트 루프 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaHb7TUjXyD0"
      },
      "source": [
        "@tf.function\n",
        "def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuarcy):\n",
        "  # labels는 <sos>, <eos> 를 포함한 정보\n",
        "  # output_labels : <sos>를 제외하고 <eos>를 포함해서 만든다.\n",
        "  output_labels = labels[:, 1:]\n",
        "  # shifted_lables : <sos>를 포함하고 <eos>를 제외해서 만든다.\n",
        "  shifted_labels = labels[:, :-1]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # inputs : x의 역할. Encoder에 들어감\n",
        "    # shifted_labels : Encoder가 예측하고, 예측해야 할 데이터\n",
        "    predictions = model([inputs, shifted_labels], training=True) # 예측을 하고\n",
        "    loss = loss_object(output_labels, predictions) # 정답이 이거였어~ 라고 이야기 하는 것\n",
        "  \n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(output_labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(model, inputs):\n",
        "  # 입력 데이터만 주고 추론은 모델이 알아서 할 수 있도록...\n",
        "  return model(inputs, training=False)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vpAEXCwjUUc"
      },
      "source": [
        "# 데이터셋 준비\n",
        "* http://www.aihub.or.kr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI74X4AxjgID"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "dataset_file = \"chatbot_data.csv\"\n",
        "okt = Okt()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtn8B48Pj0CF"
      },
      "source": [
        "with open(dataset_file, 'r') as file:\n",
        "  lines = file.readlines()\n",
        "  seq = [\" \".join(okt.morphs(line)) for line in lines]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxYPfEHzkHs_",
        "outputId": "0b8efbec-9dfa-48d3-c4a5-4c3108543add"
      },
      "source": [
        "seq[:6]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['아이스 아메리카노 하나요 \\n',\n",
              " '테이크아웃 하실 건가 요 ? \\n',\n",
              " '저 카푸치노 로 주문 할게요 \\n',\n",
              " '시럽 은 얼마나 뿌려 드릴 까요 ? \\n',\n",
              " '저 도장 다 모았는데 나중 에 써도 되나요 ? \\n',\n",
              " '네 다음 에 써도 됩니다 \\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNXC6bISkNty",
        "outputId": "0fdcb3ad-e1f4-4da5-b964-1fbd0fb5ad81"
      },
      "source": [
        "questions = seq[::2]\n",
        "answers = [\"\\t \" + lines for lines in seq[1::2]] # \\t : <sos>\n",
        "\n",
        "print(questions[:3])\n",
        "print(answers[:3])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['아이스 아메리카노 하나요 \\n', '저 카푸치노 로 주문 할게요 \\n', '저 도장 다 모았는데 나중 에 써도 되나요 ? \\n']\n",
            "['\\t 테이크아웃 하실 건가 요 ? \\n', '\\t 시럽 은 얼마나 뿌려 드릴 까요 ? \\n', '\\t 네 다음 에 써도 됩니다 \\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYE0RwZMlJZC"
      },
      "source": [
        "# 데이터 잘라내기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dZxVwIallFd",
        "outputId": "b3807f94-2737-4ae2-e389-271817bfd8ea"
      },
      "source": [
        "num_samples = len(questions)\n",
        "print(num_samples)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uj_CQuOlr4q",
        "outputId": "dcf1540e-d409-4f9c-c39b-afb9b128fd6a"
      },
      "source": [
        "term = list(range(num_samples))\n",
        "print(\"섞이기 전 : {}\".format(term[:10]))\n",
        "# 랜덤 시드 고정\n",
        "random.seed(0)\n",
        "random.shuffle(term)\n",
        "\n",
        "print(\"섞인 후 : {}\".format(term[:10]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "섞이기 전 : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "섞인 후 : [419, 459, 130, 431, 370, 26, 201, 56, 366, 108]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlZc0irqmBZv"
      },
      "source": [
        "* questions : 입력 데이터(inputs)\n",
        "* answers : 예측 레이블 (outputs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP4HCL5KmcXa"
      },
      "source": [
        "train_q = [] # X_train\n",
        "train_a = [] # y_train\n",
        "\n",
        "test_q = [] # X_test\n",
        "test_a = [] # y_test"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iSPW7pkm5Gr"
      },
      "source": [
        "test_ratio = 0.2\n",
        "test_cnt = int(len(questions) * test_ratio)\n",
        "\n",
        "train_indices = term[test_cnt: ]\n",
        "test_indices  = term[:test_cnt]\n",
        "\n",
        "for idx in train_indices:\n",
        "  train_q.append(questions[idx])\n",
        "  train_a.append(answers[idx])\n",
        "\n",
        "for idx in test_indices:\n",
        "  test_q.append(questions[idx])\n",
        "  test_a.append(answers[idx])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Scw4i61Nm_TW",
        "outputId": "6c91c80b-1b8c-4ec5-d44c-2a00c78a573f"
      },
      "source": [
        "test_q[:3], test_a[:3]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['사이 즈 업 해서 주세요 \\n',\n",
              "  '캐러멜 드리블 이랑 통 잡아 칩이요 \\n',\n",
              "  '시즌 메뉴 와 함께 구성 되어 있는 세트 메뉴 가 있나요 ? \\n'],\n",
              " ['\\t 네 결제 는 어떻게 도 와 드릴 까요 ? \\n',\n",
              "  '\\t 6700원 결제 도 와 드리겠습니다 \\n',\n",
              "  '\\t 네 치즈 케이크 와 시즌 메뉴 두 잔 으로 구성 된 세트 메뉴 있습니다 \\n'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drXeByqjoLUB"
      },
      "source": [
        "# 토크나이징"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bg1oLpLonN3"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t78SQd6pKFo",
        "outputId": "442ddca6-2bc8-48e5-c1cf-02f8c2d6e376"
      },
      "source": [
        "tokenizer.fit_on_texts(train_q + train_a) # 질문과 대답의 모든 내용을 토큰화\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\n': 1, '\\t': 2, '네': 3, '주세요': 4, '로': 5, '아메리카노': 6, '는': 7, '에': 8, '아이스': 9, '도': 10, '요': 11, '잔': 12, '이': 13, '한': 14, '드릴': 15, '까요': 16, '은': 17, '입니다': 18, '사이즈': 19, '가': 20, '있나요': 21, '결제': 22, '수': 23, '하나': 24, '있습니다': 25, '와': 26, '드시고': 27, '해주세요': 28, '할게요': 29, '으로': 30, '라테': 31, '추가': 32, '따뜻한': 33, '주문': 34, '사용': 35, '음료': 36, '되나요': 37, '여기': 38, '아니요': 39, '거': 40, '얼마': 41, '개': 42, '그럼': 43, '카드': 44, '랑': 45, '드리겠습니다': 46, '케이크': 47, '어떤': 48, '걸': 49, '포인트': 50, '가시나요': 51, '한잔': 52, '할인': 53, '적립': 54, '다': 55, '커피': 56, '더': 57, '인가요': 58, '쿠폰': 59, '가요': 60, '드릴게요': 61, '티': 62, '건': 63, '가능합니다': 64, '알겠습니다': 65, '에서': 66, '가능한가요': 67, '매장': 68, '를': 69, '진동': 70, '면': 71, '벨': 72, '안': 73, '번호': 74, '만': 75, '에요': 76, '메뉴': 77, '하나요': 78, '디카': 79, '페인': 80, '건가': 81, '샷': 82, '있어요': 83, '됩니다': 84, '테이크아웃': 85, '예요': 86, '스무디': 87, '게': 88, '카페라테': 89, '두': 90, '같이': 91, '자몽': 92, '하고': 93, '치즈케이크': 94, '제일': 95, '뭐': 96, '카페모카': 97, '기프티콘': 98, '세트': 99, '지금': 100, '종류': 101, '해서': 102, '업': 103, '먹고': 104, '휘핑크림': 105, '이랑': 106, '머핀': 107, '몇': 108, '어떻게': 109, '현금영수증': 110, '원': 111, '해드리겠습니다': 112, '하실': 113, '까지': 114, '초코': 115, '주스': 116, '화이트': 117, '프라푸치노': 118, '해': 119, '시럽': 120, '테이크': 121, '아웃': 122, '많이': 123, '베이글': 124, '2': 125, '다른': 126, '울리면': 127, '가능하세요': 128, '1': 129, '텀블러': 130, '저': 131, '갈': 132, '컵': 133, '주시': 134, '할': 135, '바닐라': 136, '크림': 137, '추천': 138, '없으신': 139, '4500원': 140, '해드릴게요': 141, '500원': 142, '총': 143, '포장': 144, '딸기': 145, '중': 146, '혹시': 147, '판매': 148, '밀크': 149, '영수증': 150, '과': 151, '둘': 152, '스콘': 153, '아': 154, '잘': 155, '찍어주세요': 156, '계산': 157, '루이보스': 158, '정도': 159, '맛': 160, '원두': 161, '필요한': 162, '올려': 163, '가실': 164, '준비': 165, '없으세요': 166, '어디': 167, '자리': 168, '얼마나': 169, '오늘': 170, '휘핑': 171, '얼음': 172, '그냥': 173, '화장실': 174, '담아': 175, '서': 176, '라지': 177, '있을까요': 178, '과일': 179, '카페인': 180, '것': 181, '가격': 182, '인데': 183, '있으세요': 184, '있으신': 185, '저희': 186, '치즈': 187, '플랫': 188, '죠': 189, '시즌': 190, '넣어': 191, '고객': 192, '님': 193, '알려': 194, '도장': 195, '되죠': 196, '그러면': 197, '티라미슈': 198, '변경': 199, '페이': 200, '데': 201, '카푸치노': 202, '주실': 203, '고': 204, '샌드위치': 205, '티라미수': 206, '현금': 207, '그렇게': 208, '쿠키': 209, '차갑게': 210, '아니오': 211, '일회용': 212, '따뜻하게': 213, '무료': 214, '이나': 215, '말차': 216, '나가요': 217, '영업': 218, '있는데': 219, '생크림': 220, '이드': 221, '을': 222, '유자차': 223, '조각': 224, '아뇨': 225, '유리잔': 226, '뭘': 227, '와주세요': 228, '10시': 229, '필요하신': 230, '해드릴까': 231, '가능하십니다': 232, '가세': 233, '말씀': 234, '완료': 235, '되었습니다': 236, '하시겠어요': 237, '선택': 238, '부탁드릴게요': 239, '때': 240, '핫초코': 241, '파나요': 242, '블루베리': 243, '담아주세요': 244, '건데': 245, '차가운': 246, '엔': 247, '뭔가': 248, '주': 249, '주시나요': 250, '넣어주세요': 251, '찾으러': 252, '될까': 253, '우유': 254, '부탁드려요': 255, '담아주실': 256, '스몰': 257, '10': 258, '가능할까': 259, '괜찮아요': 260, '키위': 261, '주차': 262, '조금': 263, '없나요': 264, '에스프레소': 265, '입력': 266, '카페라떼': 267, '페퍼민트': 268, '그리고': 269, '멤버십': 270, '앞': 271, '피지': 272, '제': 273, '녹차': 274, '바코드': 275, '와이파이': 276, '비밀번호': 277, '차': 278, '만들어': 279, '캐리어': 280, '겨울': 281, '시': 282, '있는': 283, '에는': 284, '쪽': 285, '잠시': 286, '10분': 287, '에서는': 288, '머그컵': 289, '않습니다': 290, '그럼요': 291, '하시면': 292, '하시는': 293, '가능해요': 294, '4000원': 295, '나': 296, '맞으세요': 297, '2000원': 298, '오시': 299, '레드': 300, '벨벳': 301, '언제': 302, '맛있는': 303, '모았는데': 304, '써도': 305, '하겠습니다': 306, '카카오': 307, '사이': 308, '즈': 309, '기다려야': 310, '의': 311, '개인': 312, '칩': 313, '적게': 314, '그': 315, '팥빙수': 316, '아이리쉬': 317, '있죠': 318, '갈게요': 319, '허니': 320, '브레드': 321, '직접': 322, '우려': 323, '시간': 324, '계절': 325, '부탁드립니다': 326, '모카': 327, '아아': 328, '만들어주세요': 329, '차는': 330, '캐러멜': 331, '들고': 332, '통신사': 333, '나갈': 334, '딸기스무디': 335, '생': 336, '모으면': 337, '마실': 338, '양': 339, '다음': 340, '번': 341, '이번': 342, '좀': 343, '하시나요': 344, '라떼': 345, '잔이요': 346, '걸려요': 347, '주시겠어요': 348, '어니언': 349, '접시': 350, '빵': 351, '민트': 352, '올려주세요': 353, '그건': 354, '쓸': 355, '하면': 356, '프라': 357, '페': 358, '맛있어요': 359, '걸릴까': 360, '충전': 361, '따로': 362, '있고': 363, '뜨거운': 364, '감귤': 365, '인절미': 366, '쏙쏙': 367, '붕어빵': 368, '고소한': 369, '품절': 370, '인': 371, '엘지': 372, '파이': 373, '전': 374, '대체': 375, '무엇': 376, '두유': 377, '없습니다': 378, '잘나가요': 379, '불가능합니다': 380, '괜찮으세요': 381, '300원': 382, '기다려주세요': 383, '내': 384, '상': 385, '드리고': 386, '이신': 387, '재료': 388, '바로': 389, '손님': 390, '하지': 391, '안됩니다': 392, '돼요': 393, '블랙': 394, '찍어': 395, '괜찮으신': 396, '픽업': 397, '해드려요': 398, '제공': 399, '오세요': 400, '받았습니다': 401, '플레인': 402, '기본': 403, '오후': 404, '따듯': 405, '안되고': 406, '톨': 407, '데워': 408, '5분': 409, '되셨습니다': 410, '9500원': 411, '카운터': 412, '문': 413, '없으시고요': 414, '보다': 415, '드려요': 416, '머그잔': 417, '해주시면': 418, '바꿔': 419, '5000원': 420, '점': 421, '부터': 422, '마감': 423, '앉아있다': 424, '가면': 425, '잘나가는': 426, '가져갈': 427, '기프트': 428, '콘': 429, '가능하나요': 430, '브런치': 431, '가져갈게요': 432, '빨대': 433, '아메리카': 434, '부탁': 435, '해요': 436, '나오나요': 437, '나중': 438, 'kt': 439, '더블': 440, '위': 441, '마시다가': 442, '들고나': 443, '그란': 444, '사이드': 445, '날씨': 446, '먹어요': 447, '받으려면': 448, '30': 449, '자바': 450, '빼고요': 451, '량': 452, '조절': 453, '걸리나요': 454, '십': 455, '시오': 456, '마카롱': 457, '남아있나요': 458, '하려고': 459, '대면': 460, '데워주실': 461, '줘요': 462, '남은': 463, '삼': 464, '성': 465, '에만': 466, '나오는': 467, '후': 468, '오면': 469, '받을': 470, '타서': 471, '연하게': 472, '만들어주실': 473, '모바일': 474, '함께': 475, '드리블': 476, '통': 477, '잡아': 478, '깔아주세요': 479, '전부': 480, '돼도': 481, '해주죠': 482, '안전히': 483, '있게': 484, '마시다': 485, '거니': 486, '없을까요': 487, '아이스커피': 488, '가능하죠': 489, '적은': 490, '숏': 491, '했어요': 492, '업은': 493, '삼성': 494, '덜': 495, '넣어주실': 496, '그린': 497, '20': 498, '말고': 499, '새로': 500, '적고': 501, '사람': 502, '당': 503, '씩': 504, '시켜야': 505, '아닌': 506, '들어간': 507, '휴대폰': 508, '마시려는데': 509, '명': 510, '창가': 511, '앉을': 512, '들어있지': 513, '않은': 514, '차감': 515, '으로는': 516, '없이': 517, '줄': 518, '스타벅스': 519, '당근': 520, '캐머': 521, '마일': 522, '물티슈': 523, '챙겨': 524, '큰': 525, '쿠앤크': 526, '치노': 527, '미지근한': 528, '물': 529, '그거': 530, '저번': 531, '와서': 532, '먹은': 533, '미숫가루': 534, '없네요': 535, '4': 536, '아직': 537, '런치': 538, '코': 539, '찡할': 540, '만큼': 541, '동시': 542, '가겠습니다': 543, '달': 544, '지': 545, '않나요': 546, '마키아토': 547, '들어가죠': 548, '먹을': 549, '맴버쉽': 550, '옮겨줄': 551, '콘센트': 552, '주시는데': 553, '들어가있나요': 554, '키': 555, '오스': 556, '크에서': 557, '되네요': 558, '나왔어요': 559, '해주신': 560, '3': 561, '어우러지는': 562, '업도': 563, '진하게': 564, '젤': 565, '좋아하는': 566, '리저브': 567, '처음': 568, '눌렀습니다': 569, '쓴가요': 570, '기존': 571, '통합': 572, '시킬': 573, '수도': 574, '레몬': 575, '분': 576, '들은': 577, '기계': 578, '인식': 579, '안되는데요': 580, '2만': 581, '밸': 582, '휘핑빼': 583, '우선': 584, '물이': 585, '치': 586, '오천': 587, '이요': 588, '먹을게요': 589, '감사합니다': 590, '하나같이': 591, '빅사': 592, '이즈': 593, '기프트카드': 594, '올라가는': 595, '작은': 596, '실내': 597, '가져왔는데': 598, '평소': 599, '주로': 600, '마시는데요': 601, '그런데': 602, '중간': 603, '대신': 604, '유로': 605, '빼주세요': 606, '빼주실': 607, '너무': 608, '달아서': 609, '싫은데': 610, '있으면': 611, '케익': 612, '교환': 613, '권': 614, '바꿀': 615, '로는': 616, '맛있나요': 617, '오픈': 618, '챙겨주세요': 619, '단것': 620, '별로': 621, '와는': 622, '해주시': 623, '카페': 624, '요금': 625, '바꿔서': 626, '올해': 627, '부터는': 628, '점포': 629, '1회': 630, '용을': 631, '바닥': 632, '청소': 633, '해야': 634, '죄송합니다': 635, '요새': 636, '라이트': 637, '5500원': 638, '바나나': 639, '파인애플': 640, '가지': 641, '계시다가': 642, '가져가세요': 643, '레귤러': 644, '14': 645, '나갑니다': 646, '팔려요': 647, '적용': 648, '9300원': 649, '규정': 650, '인기': 651, '기다려': 652, '단체': 653, '작아서': 654, '들어가요': 655, '6600원': 656, '팔렸어요': 657, '음식': 658, '이라': 659, '안되세요': 660, '되어': 661, '1만': 662, '1천': 663, '복도': 664, '나가시': 665, '보여요': 666, '가시는': 667, '괜찮으실까': 668, '8000원': 669, '드립니다': 670, '왼쪽': 671, '보시': 672, '7천원': 673, '만들고': 674, '진할': 675, '다시': 676, '해주시겠어요': 677, '드실': 678, '드리도록': 679, '되어있습니다': 680, '보여주세요': 681, '손잡이': 682, '투': 683, '들어가는데': 684, '대로': 685, '사항': 686, '곳': 687, '고르시면': 688, '보통': 689, '여자': 690, '분들': 691, '특정': 692, '선호': 693, '아니면': 694, '예': 695, '가체': 696, '프': 697, '냉동': 698, '제품': 699, '이고': 700, '다해': 701, '10900원': 702, '하프': 703, '되셔서': 704, '또': 705, '50': 706, '4천원': 707, '18000원': 708, '꽃': 709, '15000': 710, '1500원': 711, '어떠세요': 712, '500': 713, '기다리시면': 714, '안내': 715, '성분': 716, '들어가지': 717, '번만': 718, '테이블': 719, '하게': 720, '아예': 721, '빼는': 722, '최대한': 723, '하시고': 724, '잔액': 725, '3천': 726, '드렸고': 727, '이리': 728, '드릴가요': 729, '하세요': 730, '오전': 731, '이전': 732, '오셔서': 733, '한정': 734, '라': 735, '동일하게': 736, '저기': 737, '가져오시면': 738, '2시': 739, '가지러': 740, '곡물': 741, '가루': 742, '넣어서': 743, '고소하고': 744, '받으러': 745, '세': 746, '들어갑니다': 747, '호두': 748, '어울려요': 749, '적어주세요': 750, '층': 751, '나오면': 752, '로만': 753, '앉아': 754, '계시': 755, '가져다': 756, '기다리세요': 757, '오른쪽': 758, '벽쪽': 759, '건물': 760, '뒤': 761, '주차장': 762, '들어가있습니다': 763, '휴대': 764, '알려주시면': 765, '가능합니다만': 766, '천': 767, '올라가게': 768, '상큼': 769, '어울립니다': 770, '죄송합니다만': 771, '방금': 772, '떨어졌어요': 773, '유효': 774, '기간': 775, '라면': 776, '딱': 777, '맞게': 778, '오셨네요': 779, '원하시는': 780, '내리는': 781, '방법': 782, '대략': 783, '걸립니다': 784, '고맙습니다': 785, '쓴맛': 786, '없고': 787, '산미': 788, '들': 789, '향': 790, '나죠': 791, '합쳐': 792, '꽂아': 793, '6000원': 794, '이에요': 795, '밀려서': 796, '7분': 797, '걸릴': 798, '앞쪽': 799, '눌러주세요': 800, '불러': 801, '뜨거운건': 802, '할까': 803, '다모아': 804, '커피한잔': 805, '필요하세요': 806, '해드려도': 807, '괜찮을까요': 808, '떨어져서': 809, '어렵습니다': 810, '신': 811, '꺼': 812, '되는': 813, '8800원': 814, '해드렸습니다': 815, '정책': 816, '불가능하고': 817, '나가실': 818, '하시겠습니까': 819, '필요': 820, '다른거': 821, '필요한거': 822, '현재': 823, '법적': 824, '금지': 825, '종이': 826, '써져있습니다': 827, '한테': 828, '보여주시고': 829, '확인': 830, '버튼': 831, '누르면': 832, '요즘': 833, '없어요': 834, '다크': 835, '초콜릿': 836, '어우러진': 837, '달콤한': 838, '하셨나요': 839, '죄송하지만': 840, '과테말라': 841, '케냐': 842, '섞은': 843, '마이크로': 844, '블렌드': 845, '5천': 846, '필요하신가요': 847, '되세요': 848, '가시는거세요': 849, '7000원': 850, '포크': 851, '들어갔어요': 852, '4100원': 853, '2500원': 854, '당연하죠': 855, '팔려서요': 856, '나온': 857, '오곡': 858, '5000': 859, '그중': 860, '4500': 861, '가능한데': 862, '이상': 863, '9시': 864, '열어요': 865, '12시': 866, '까지라': 867, '드리면': 868, '되셨고': 869, '아주': 870, '단맛': 871, '아닌데요': 872, '라임': 873, '하단': 874, '적혀있습니다': 875, '합니다': 876, '되면': 877}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyX1EUQepYJZ"
      },
      "source": [
        "정수 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3k-H27bqG4_",
        "outputId": "888b7913-da66-4fdb-ea75-5a01fe44fb3b"
      },
      "source": [
        "train_q_seq = tokenizer.texts_to_sequences(train_q)\n",
        "train_a_seq = tokenizer.texts_to_sequences(train_a)\n",
        "\n",
        "test_q_seq  = tokenizer.texts_to_sequences(test_q)\n",
        "test_a_seq  = tokenizer.texts_to_sequences(test_a)\n",
        "\n",
        "train_q_seq[:3], train_a_seq[:3]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[85, 12, 30, 4, 1], [3, 239, 1], [3, 300, 301, 47, 4, 1]],\n",
              " [[2, 627, 628, 629, 73, 66, 630, 631, 35, 113, 23, 378, 1],\n",
              "  [2, 57, 162, 63, 139, 60, 1],\n",
              "  [2, 36, 7, 227, 5, 15, 16, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNxTApmaqa8a"
      },
      "source": [
        "패딩 후 최종 데이터 마련하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mzzXyq2quqx"
      },
      "source": [
        "# 문장의 최대길이 64로 설정 했음!\n",
        "X_train = pad_sequences(\n",
        "    train_q_seq,\n",
        "    value=0,\n",
        "    padding='pre',\n",
        "    maxlen=64\n",
        ")\n",
        "\n",
        "y_train = pad_sequences(\n",
        "    train_a_seq,\n",
        "    value=0,\n",
        "    padding='post',\n",
        "    maxlen=65 # <sos>, <eos>\n",
        ")\n",
        "\n",
        "X_test = pad_sequences( test_q_seq, value=0, padding='pre', maxlen=64 )\n",
        "y_test = pad_sequences( test_a_seq, value=0, padding='post', maxlen=65 )"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_putRvhsNA-",
        "outputId": "0168fea0-508a-423c-c397-4507fa89ea48"
      },
      "source": [
        "X_train[0], y_train[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0, 85, 12, 30,  4,  1], dtype=int32),\n",
              " array([  2, 627, 628, 629,  73,  66, 630, 631,  35, 113,  23, 378,   1,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "       dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-K8I7iJsQIR"
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1024).batch(32).prefetch(1024) # prefetch : 데이터를 미리 저장할 공간을 의미\n",
        "test_ds  = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(1).prefetch(1024)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTEFKuXXtszt"
      },
      "source": [
        "# 학습 환경 정의\n",
        "모델 생성, 손실 함수, 최적화 알고리즘, 평가지표 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqnsbykHwiuV"
      },
      "source": [
        "# 모델 생성\n",
        "model = Seq2seq(\n",
        "    sos=tokenizer.word_index[\"\\t\"],\n",
        "    eos=tokenizer.word_index[\"\\n\"]\n",
        ")\n",
        "\n",
        "# Loss 선정. 정수 인코딩된 결과를 t로 사용, softmax 이용한 정수값을 예측으로 쓰니까 sparse_categorical_crossentropy\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# 모델 평가 방식\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcnZVaa-yb_-"
      },
      "source": [
        "# 학습 루프 동작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4RHBVRVyhut",
        "outputId": "20cf11ba-30a6-45a3-8a20-f73fc242c930"
      },
      "source": [
        "EPOCHS = 200\n",
        "for epoch in range(EPOCHS):\n",
        "  for seqs, labels in train_ds:\n",
        "    train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy)\n",
        "  \n",
        "  print(\"Epoch : {}, Loss : {:.3f}, Accuracy : {:.3f}\".format(epoch + 1,\n",
        "                                                      train_loss.result(),\n",
        "                                                      train_accuracy.result() * 100))\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 1, Loss : 2.777, Accuracy : 83.184\n",
            "Epoch : 2, Loss : 0.621, Accuracy : 90.590\n",
            "Epoch : 3, Loss : 0.576, Accuracy : 90.977\n",
            "Epoch : 4, Loss : 0.557, Accuracy : 91.113\n",
            "Epoch : 5, Loss : 0.539, Accuracy : 91.152\n",
            "Epoch : 6, Loss : 0.538, Accuracy : 91.172\n",
            "Epoch : 7, Loss : 0.528, Accuracy : 91.219\n",
            "Epoch : 8, Loss : 0.512, Accuracy : 91.344\n",
            "Epoch : 9, Loss : 0.502, Accuracy : 91.336\n",
            "Epoch : 10, Loss : 0.491, Accuracy : 91.461\n",
            "Epoch : 11, Loss : 0.468, Accuracy : 91.844\n",
            "Epoch : 12, Loss : 0.457, Accuracy : 92.199\n",
            "Epoch : 13, Loss : 0.452, Accuracy : 92.230\n",
            "Epoch : 14, Loss : 0.440, Accuracy : 92.375\n",
            "Epoch : 15, Loss : 0.431, Accuracy : 92.457\n",
            "Epoch : 16, Loss : 0.427, Accuracy : 92.527\n",
            "Epoch : 17, Loss : 0.419, Accuracy : 92.613\n",
            "Epoch : 18, Loss : 0.411, Accuracy : 92.727\n",
            "Epoch : 19, Loss : 0.401, Accuracy : 92.730\n",
            "Epoch : 20, Loss : 0.400, Accuracy : 92.777\n",
            "Epoch : 21, Loss : 0.392, Accuracy : 92.891\n",
            "Epoch : 22, Loss : 0.393, Accuracy : 92.926\n",
            "Epoch : 23, Loss : 0.385, Accuracy : 92.973\n",
            "Epoch : 24, Loss : 0.381, Accuracy : 93.047\n",
            "Epoch : 25, Loss : 0.371, Accuracy : 93.090\n",
            "Epoch : 26, Loss : 0.368, Accuracy : 93.148\n",
            "Epoch : 27, Loss : 0.362, Accuracy : 93.145\n",
            "Epoch : 28, Loss : 0.355, Accuracy : 93.211\n",
            "Epoch : 29, Loss : 0.353, Accuracy : 93.215\n",
            "Epoch : 30, Loss : 0.350, Accuracy : 93.254\n",
            "Epoch : 31, Loss : 0.344, Accuracy : 93.352\n",
            "Epoch : 32, Loss : 0.337, Accuracy : 93.379\n",
            "Epoch : 33, Loss : 0.331, Accuracy : 93.477\n",
            "Epoch : 34, Loss : 0.327, Accuracy : 93.492\n",
            "Epoch : 35, Loss : 0.320, Accuracy : 93.531\n",
            "Epoch : 36, Loss : 0.322, Accuracy : 93.641\n",
            "Epoch : 37, Loss : 0.311, Accuracy : 93.641\n",
            "Epoch : 38, Loss : 0.310, Accuracy : 93.660\n",
            "Epoch : 39, Loss : 0.305, Accuracy : 93.707\n",
            "Epoch : 40, Loss : 0.300, Accuracy : 93.730\n",
            "Epoch : 41, Loss : 0.292, Accuracy : 93.805\n",
            "Epoch : 42, Loss : 0.289, Accuracy : 93.887\n",
            "Epoch : 43, Loss : 0.288, Accuracy : 93.930\n",
            "Epoch : 44, Loss : 0.278, Accuracy : 93.973\n",
            "Epoch : 45, Loss : 0.276, Accuracy : 94.004\n",
            "Epoch : 46, Loss : 0.270, Accuracy : 94.051\n",
            "Epoch : 47, Loss : 0.267, Accuracy : 94.148\n",
            "Epoch : 48, Loss : 0.257, Accuracy : 94.254\n",
            "Epoch : 49, Loss : 0.255, Accuracy : 94.328\n",
            "Epoch : 50, Loss : 0.251, Accuracy : 94.438\n",
            "Epoch : 51, Loss : 0.242, Accuracy : 94.531\n",
            "Epoch : 52, Loss : 0.235, Accuracy : 94.684\n",
            "Epoch : 53, Loss : 0.233, Accuracy : 94.777\n",
            "Epoch : 54, Loss : 0.225, Accuracy : 94.973\n",
            "Epoch : 55, Loss : 0.219, Accuracy : 94.977\n",
            "Epoch : 56, Loss : 0.212, Accuracy : 95.184\n",
            "Epoch : 57, Loss : 0.208, Accuracy : 95.293\n",
            "Epoch : 58, Loss : 0.201, Accuracy : 95.480\n",
            "Epoch : 59, Loss : 0.195, Accuracy : 95.668\n",
            "Epoch : 60, Loss : 0.187, Accuracy : 95.785\n",
            "Epoch : 61, Loss : 0.184, Accuracy : 95.852\n",
            "Epoch : 62, Loss : 0.177, Accuracy : 95.977\n",
            "Epoch : 63, Loss : 0.170, Accuracy : 96.129\n",
            "Epoch : 64, Loss : 0.165, Accuracy : 96.293\n",
            "Epoch : 65, Loss : 0.159, Accuracy : 96.418\n",
            "Epoch : 66, Loss : 0.153, Accuracy : 96.484\n",
            "Epoch : 67, Loss : 0.146, Accuracy : 96.766\n",
            "Epoch : 68, Loss : 0.140, Accuracy : 96.871\n",
            "Epoch : 69, Loss : 0.134, Accuracy : 96.945\n",
            "Epoch : 70, Loss : 0.129, Accuracy : 97.074\n",
            "Epoch : 71, Loss : 0.123, Accuracy : 97.230\n",
            "Epoch : 72, Loss : 0.117, Accuracy : 97.324\n",
            "Epoch : 73, Loss : 0.112, Accuracy : 97.543\n",
            "Epoch : 74, Loss : 0.108, Accuracy : 97.637\n",
            "Epoch : 75, Loss : 0.104, Accuracy : 97.777\n",
            "Epoch : 76, Loss : 0.100, Accuracy : 97.836\n",
            "Epoch : 77, Loss : 0.097, Accuracy : 97.949\n",
            "Epoch : 78, Loss : 0.092, Accuracy : 98.004\n",
            "Epoch : 79, Loss : 0.090, Accuracy : 98.125\n",
            "Epoch : 80, Loss : 0.086, Accuracy : 98.188\n",
            "Epoch : 81, Loss : 0.083, Accuracy : 98.301\n",
            "Epoch : 82, Loss : 0.080, Accuracy : 98.312\n",
            "Epoch : 83, Loss : 0.076, Accuracy : 98.379\n",
            "Epoch : 84, Loss : 0.075, Accuracy : 98.500\n",
            "Epoch : 85, Loss : 0.069, Accuracy : 98.641\n",
            "Epoch : 86, Loss : 0.067, Accuracy : 98.633\n",
            "Epoch : 87, Loss : 0.064, Accuracy : 98.762\n",
            "Epoch : 88, Loss : 0.061, Accuracy : 98.805\n",
            "Epoch : 89, Loss : 0.058, Accuracy : 98.867\n",
            "Epoch : 90, Loss : 0.055, Accuracy : 98.945\n",
            "Epoch : 91, Loss : 0.053, Accuracy : 98.973\n",
            "Epoch : 92, Loss : 0.051, Accuracy : 99.035\n",
            "Epoch : 93, Loss : 0.049, Accuracy : 99.031\n",
            "Epoch : 94, Loss : 0.047, Accuracy : 99.086\n",
            "Epoch : 95, Loss : 0.046, Accuracy : 99.172\n",
            "Epoch : 96, Loss : 0.044, Accuracy : 99.199\n",
            "Epoch : 97, Loss : 0.043, Accuracy : 99.199\n",
            "Epoch : 98, Loss : 0.041, Accuracy : 99.199\n",
            "Epoch : 99, Loss : 0.040, Accuracy : 99.277\n",
            "Epoch : 100, Loss : 0.038, Accuracy : 99.309\n",
            "Epoch : 101, Loss : 0.037, Accuracy : 99.336\n",
            "Epoch : 102, Loss : 0.035, Accuracy : 99.328\n",
            "Epoch : 103, Loss : 0.034, Accuracy : 99.410\n",
            "Epoch : 104, Loss : 0.032, Accuracy : 99.418\n",
            "Epoch : 105, Loss : 0.031, Accuracy : 99.477\n",
            "Epoch : 106, Loss : 0.031, Accuracy : 99.406\n",
            "Epoch : 107, Loss : 0.030, Accuracy : 99.453\n",
            "Epoch : 108, Loss : 0.029, Accuracy : 99.484\n",
            "Epoch : 109, Loss : 0.026, Accuracy : 99.551\n",
            "Epoch : 110, Loss : 0.025, Accuracy : 99.566\n",
            "Epoch : 111, Loss : 0.024, Accuracy : 99.625\n",
            "Epoch : 112, Loss : 0.023, Accuracy : 99.668\n",
            "Epoch : 113, Loss : 0.022, Accuracy : 99.609\n",
            "Epoch : 114, Loss : 0.021, Accuracy : 99.652\n",
            "Epoch : 115, Loss : 0.020, Accuracy : 99.719\n",
            "Epoch : 116, Loss : 0.020, Accuracy : 99.676\n",
            "Epoch : 117, Loss : 0.020, Accuracy : 99.707\n",
            "Epoch : 118, Loss : 0.019, Accuracy : 99.715\n",
            "Epoch : 119, Loss : 0.020, Accuracy : 99.672\n",
            "Epoch : 120, Loss : 0.021, Accuracy : 99.688\n",
            "Epoch : 121, Loss : 0.021, Accuracy : 99.664\n",
            "Epoch : 122, Loss : 0.022, Accuracy : 99.648\n",
            "Epoch : 123, Loss : 0.021, Accuracy : 99.676\n",
            "Epoch : 124, Loss : 0.020, Accuracy : 99.672\n",
            "Epoch : 125, Loss : 0.018, Accuracy : 99.734\n",
            "Epoch : 126, Loss : 0.016, Accuracy : 99.766\n",
            "Epoch : 127, Loss : 0.015, Accuracy : 99.820\n",
            "Epoch : 128, Loss : 0.014, Accuracy : 99.766\n",
            "Epoch : 129, Loss : 0.013, Accuracy : 99.809\n",
            "Epoch : 130, Loss : 0.012, Accuracy : 99.844\n",
            "Epoch : 131, Loss : 0.013, Accuracy : 99.844\n",
            "Epoch : 132, Loss : 0.016, Accuracy : 99.758\n",
            "Epoch : 133, Loss : 0.016, Accuracy : 99.809\n",
            "Epoch : 134, Loss : 0.014, Accuracy : 99.824\n",
            "Epoch : 135, Loss : 0.012, Accuracy : 99.859\n",
            "Epoch : 136, Loss : 0.011, Accuracy : 99.887\n",
            "Epoch : 137, Loss : 0.010, Accuracy : 99.891\n",
            "Epoch : 138, Loss : 0.010, Accuracy : 99.891\n",
            "Epoch : 139, Loss : 0.009, Accuracy : 99.906\n",
            "Epoch : 140, Loss : 0.009, Accuracy : 99.914\n",
            "Epoch : 141, Loss : 0.008, Accuracy : 99.906\n",
            "Epoch : 142, Loss : 0.008, Accuracy : 99.918\n",
            "Epoch : 143, Loss : 0.008, Accuracy : 99.914\n",
            "Epoch : 144, Loss : 0.007, Accuracy : 99.918\n",
            "Epoch : 145, Loss : 0.007, Accuracy : 99.922\n",
            "Epoch : 146, Loss : 0.007, Accuracy : 99.922\n",
            "Epoch : 147, Loss : 0.007, Accuracy : 99.934\n",
            "Epoch : 148, Loss : 0.007, Accuracy : 99.941\n",
            "Epoch : 149, Loss : 0.006, Accuracy : 99.941\n",
            "Epoch : 150, Loss : 0.006, Accuracy : 99.941\n",
            "Epoch : 151, Loss : 0.006, Accuracy : 99.930\n",
            "Epoch : 152, Loss : 0.006, Accuracy : 99.953\n",
            "Epoch : 153, Loss : 0.006, Accuracy : 99.934\n",
            "Epoch : 154, Loss : 0.006, Accuracy : 99.949\n",
            "Epoch : 155, Loss : 0.005, Accuracy : 99.938\n",
            "Epoch : 156, Loss : 0.005, Accuracy : 99.945\n",
            "Epoch : 157, Loss : 0.005, Accuracy : 99.949\n",
            "Epoch : 158, Loss : 0.005, Accuracy : 99.945\n",
            "Epoch : 159, Loss : 0.005, Accuracy : 99.945\n",
            "Epoch : 160, Loss : 0.005, Accuracy : 99.938\n",
            "Epoch : 161, Loss : 0.005, Accuracy : 99.949\n",
            "Epoch : 162, Loss : 0.005, Accuracy : 99.953\n",
            "Epoch : 163, Loss : 0.005, Accuracy : 99.957\n",
            "Epoch : 164, Loss : 0.004, Accuracy : 99.961\n",
            "Epoch : 165, Loss : 0.004, Accuracy : 99.953\n",
            "Epoch : 166, Loss : 0.004, Accuracy : 99.945\n",
            "Epoch : 167, Loss : 0.004, Accuracy : 99.957\n",
            "Epoch : 168, Loss : 0.004, Accuracy : 99.953\n",
            "Epoch : 169, Loss : 0.004, Accuracy : 99.961\n",
            "Epoch : 170, Loss : 0.004, Accuracy : 99.957\n",
            "Epoch : 171, Loss : 0.004, Accuracy : 99.953\n",
            "Epoch : 172, Loss : 0.004, Accuracy : 99.949\n",
            "Epoch : 173, Loss : 0.004, Accuracy : 99.961\n",
            "Epoch : 174, Loss : 0.004, Accuracy : 99.938\n",
            "Epoch : 175, Loss : 0.005, Accuracy : 99.930\n",
            "Epoch : 176, Loss : 0.004, Accuracy : 99.949\n",
            "Epoch : 177, Loss : 0.004, Accuracy : 99.934\n",
            "Epoch : 178, Loss : 0.005, Accuracy : 99.941\n",
            "Epoch : 179, Loss : 0.004, Accuracy : 99.953\n",
            "Epoch : 180, Loss : 0.004, Accuracy : 99.961\n",
            "Epoch : 181, Loss : 0.003, Accuracy : 99.957\n",
            "Epoch : 182, Loss : 0.003, Accuracy : 99.945\n",
            "Epoch : 183, Loss : 0.003, Accuracy : 99.957\n",
            "Epoch : 184, Loss : 0.003, Accuracy : 99.953\n",
            "Epoch : 185, Loss : 0.003, Accuracy : 99.953\n",
            "Epoch : 186, Loss : 0.003, Accuracy : 99.957\n",
            "Epoch : 187, Loss : 0.003, Accuracy : 99.961\n",
            "Epoch : 188, Loss : 0.003, Accuracy : 99.961\n",
            "Epoch : 189, Loss : 0.003, Accuracy : 99.953\n",
            "Epoch : 190, Loss : 0.003, Accuracy : 99.949\n",
            "Epoch : 191, Loss : 0.003, Accuracy : 99.961\n",
            "Epoch : 192, Loss : 0.003, Accuracy : 99.961\n",
            "Epoch : 193, Loss : 0.002, Accuracy : 99.965\n",
            "Epoch : 194, Loss : 0.002, Accuracy : 99.961\n",
            "Epoch : 195, Loss : 0.003, Accuracy : 99.957\n",
            "Epoch : 196, Loss : 0.002, Accuracy : 99.953\n",
            "Epoch : 197, Loss : 0.002, Accuracy : 99.949\n",
            "Epoch : 198, Loss : 0.002, Accuracy : 99.953\n",
            "Epoch : 199, Loss : 0.002, Accuracy : 99.957\n",
            "Epoch : 200, Loss : 0.002, Accuracy : 99.949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTEnxczlzPgL"
      },
      "source": [
        "Accuracy가 좋은 이유는?? `Teacher Forcing` 했으니까 좋을 수 밖에.."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14LswwakztvV"
      },
      "source": [
        "# 테스트 루프 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVwPJ34Sz3fm",
        "outputId": "5ec95835-6b3c-461a-9861-2342a52111b2"
      },
      "source": [
        "for test_seq, test_labels in test_ds:\n",
        "  prediction = test_step(model, test_seq)\n",
        "  \n",
        "  test_q = tokenizer.sequences_to_texts(test_seq.numpy()) # 질문\n",
        "  test_a = tokenizer.sequences_to_texts(test_labels.numpy()) # 실제 대답\n",
        "  test_p = tokenizer.sequences_to_texts(prediction.numpy()) # 챗봇의 대답\n",
        "\n",
        "  print(\"______\")\n",
        "  print(\"질문 : \\t{}\".format(test_q))\n",
        "  print(\"실제 대답 : {}\".format(test_a))\n",
        "  print(\"챗봇 대답 : {}\".format(test_p))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "______\n",
            "질문 : \t['사이 즈 업 해서 주세요 \\n']\n",
            "실제 대답 : ['\\t 네 결제 는 어떻게 도 와 드릴 까요 \\n']\n",
            "챗봇 대답 : ['다른 건 필요 없으신 가요 \\n']\n",
            "______\n",
            "질문 : \t['캐러멜 드리블 이랑 통 잡아 \\n']\n",
            "실제 대답 : ['\\t 결제 도 와 드리겠습니다 \\n']\n",
            "챗봇 대답 : ['네 아이스 사이즈 는 어떤 걸 로 드릴 까요 \\n']\n",
            "______\n",
            "질문 : \t['시즌 메뉴 와 함께 되어 있는 세트 메뉴 가 있나요 \\n']\n",
            "실제 대답 : ['\\t 네 치즈 케이크 와 시즌 메뉴 두 잔 으로 세트 메뉴 있습니다 \\n']\n",
            "챗봇 대답 : ['네 가능합니다 \\n']\n",
            "______\n",
            "질문 : \t['아메리카노 1 잔 주세요 \\n']\n",
            "실제 대답 : ['\\t 매장 에서 드시고 가시나요 \\n']\n",
            "챗봇 대답 : ['따뜻한 거 맞으세요 \\n']\n",
            "______\n",
            "질문 : \t['그럼 와 아이스 아메리카노 로 할게요 \\n']\n",
            "실제 대답 : ['\\t 더 필요하신 건 없나요 \\n']\n",
            "챗봇 대답 : ['네 알겠습니다 \\n']\n",
            "______\n",
            "질문 : \t['밀크 티 있나요 \\n']\n",
            "실제 대답 : ['\\t 네 있습니다 \\n']\n",
            "챗봇 대답 : ['아니요 한 사이즈 로만 판매 하고 있습니다 \\n']\n",
            "______\n",
            "질문 : \t['네 기프티콘 여기 있어요 \\n']\n",
            "실제 대답 : ['\\t 아메리카노 기프티콘 사용 되었습니다 \\n']\n",
            "챗봇 대답 : ['네 그렇게 해드리겠습니다 \\n']\n",
            "______\n",
            "질문 : \t['네 오늘 의 커피 로 주세요 \\n']\n",
            "실제 대답 : ['\\t 네 사이즈 는 어떤 걸 로 주문 넣어 드릴 까요 \\n']\n",
            "챗봇 대답 : ['드시고 가시나요 \\n']\n",
            "______\n",
            "질문 : \t['포인트 사용 없이 적립 만 할게요 \\n']\n",
            "실제 대답 : ['\\t 네 멤버십 카드 주시 면 도 와 드리겠습니다 \\n']\n",
            "챗봇 대답 : ['네 결제 되셨습니다 \\n']\n",
            "______\n",
            "질문 : \t['네 감사합니다 \\n']\n",
            "실제 대답 : ['\\t 따뜻한 카페라테 한 잔 \\n']\n",
            "챗봇 대답 : ['진동 벨 로 알려 드릴게요 \\n']\n",
            "______\n",
            "질문 : \t['네 아이스 아메리카노 한잔 주세요 \\n']\n",
            "실제 대답 : ['\\t 드시고 가실 건가 요 \\n']\n",
            "챗봇 대답 : ['드시고 가시나요 \\n']\n",
            "______\n",
            "질문 : \t['조금 만 나갈 건데 일회용 컵 에는 요 \\n']\n",
            "실제 대답 : ['\\t 거 면 컵 만 가능하세요 \\n']\n",
            "챗봇 대답 : ['네 고객 님 결제 완료 되었습니다 \\n']\n",
            "______\n",
            "질문 : \t['영업 해요 \\n']\n",
            "실제 대답 : ['\\t 영업 하고 있어요 \\n']\n",
            "챗봇 대답 : ['알겠습니다 \\n']\n",
            "______\n",
            "질문 : \t['디카 페인 아이스 아메리카노 한 잔 주세요 \\n']\n",
            "실제 대답 : ['\\t 디카 페인 아이스 아메리카노 는 기존 에 300원 추가 괜찮으신 가요 \\n']\n",
            "챗봇 대답 : ['드시고 가시나요 \\n']\n",
            "______\n",
            "질문 : \t['총 \\n']\n",
            "실제 대답 : ['\\t 스콘 두 개 아메리카노 샷 추가 텀블러 할인 해서 입니다 \\n']\n",
            "챗봇 대답 : ['휘핑크림 올려 드릴 까요 \\n']\n",
            "______\n",
            "질문 : \t['카드 로 결제 할게요 \\n']\n",
            "실제 대답 : ['\\t 네 감사합니다 \\n']\n",
            "챗봇 대답 : ['영수증 드릴게요 \\n']\n",
            "______\n",
            "질문 : \t['이 쿠키 는 뭐 예요 \\n']\n",
            "실제 대답 : ['\\t 초코 칩 쿠키 입니다 \\n']\n",
            "챗봇 대답 : ['치즈 과 동일하게 2000원 입니다 \\n']\n",
            "______\n",
            "질문 : \t['커피 에 샷 추가 가능한가요 \\n']\n",
            "실제 대답 : ['\\t 네 가능합니다 \\n']\n",
            "챗봇 대답 : ['네 고맙습니다 \\n']\n",
            "______\n",
            "질문 : \t['멤버십 카드 드리고 현금 으로 계산 할게요 \\n']\n",
            "실제 대답 : ['\\t 네 현금영수증 해드릴까 요 \\n']\n",
            "챗봇 대답 : ['현금영수증 해드릴까 요 \\n']\n",
            "______\n",
            "질문 : \t['아메리카노 톨 사이즈 로 주세요 \\n']\n",
            "실제 대답 : ['\\t 따뜻한 거 로 드릴 까요 \\n']\n",
            "챗봇 대답 : ['사이즈 는 뭘 로 드릴 까요 \\n']\n",
            "______\n",
            "질문 : \t['네 도 같이 주시겠어요 \\n']\n",
            "실제 대답 : ['\\t 네 아메리카노 4000원 입니다 \\n']\n",
            "챗봇 대답 : ['네 번호 적어주세요 \\n']\n",
            "______\n",
            "질문 : \t['핫초코 한 잔 아메리카노 사이 즈 업 한 잔 하면 얼마 인가요 \\n']\n",
            "실제 대답 : ['\\t 입니다 \\n']\n",
            "챗봇 대답 : ['들어가있습니다 \\n']\n",
            "______\n",
            "질문 : \t['쿠폰 찍어주세요 \\n']\n",
            "실제 대답 : ['\\t 네 찍어 드릴게요 \\n']\n",
            "챗봇 대답 : ['10 개 다모아 오시 면 커피한잔 드려요 \\n']\n",
            "______\n",
            "질문 : \t['조각 케이크 도 추가 해주시겠어요 \\n']\n",
            "실제 대답 : ['\\t 네 어떤 거 로 드릴 까요 \\n']\n",
            "챗봇 대답 : ['네 네 알겠습니다 \\n']\n",
            "______\n",
            "질문 : \t['네 여기 서 먹고 갈 거 예요 \\n']\n",
            "실제 대답 : ['\\t 그럼 머그잔 에 드릴게요 \\n']\n",
            "챗봇 대답 : ['네 커피 는 종이 에 써져있습니다 \\n']\n",
            "______\n",
            "질문 : \t['혹시 적립 쿠폰 으로 커피 계산 할 수 있나요 \\n']\n",
            "실제 대답 : ['\\t 네 됩니다 \\n']\n",
            "챗봇 대답 : ['네 라테 에 두유 로 변경 가능합니다 \\n']\n",
            "______\n",
            "질문 : \t['조금 마시다가 갈 거 에요 \\n']\n",
            "실제 대답 : ['\\t 가실 때 말씀 해주시면 테이크 아웃 잔 에 드릴 우선 머그잔 에 드리겠습니다 \\n']\n",
            "챗봇 대답 : ['네 포인트 해드리겠습니다 \\n']\n",
            "______\n",
            "질문 : \t['삼성 페이 되나요 \\n']\n",
            "실제 대답 : ['\\t 네 가능합니다 \\n']\n",
            "챗봇 대답 : ['네 가능하세요 \\n']\n",
            "______\n",
            "질문 : \t['플랫 화이트 라지 로 주세요 \\n']\n",
            "실제 대답 : ['\\t 네 \\n']\n",
            "챗봇 대답 : ['신 맛 과 고소한 맛 원두 중 에 어떤 걸 로 드릴 까요 \\n']\n",
            "______\n",
            "질문 : \t['아메리카노 한 잔 주세요 \\n']\n",
            "실제 대답 : ['\\t 네 아메리카노 어떤 걸 로 드릴 까요 \\n']\n",
            "챗봇 대답 : ['따뜻한 것 과 아이스 중 에 무엇 을 드릴 까요 \\n']\n",
            "______\n",
            "질문 : \t['아메리카노 기프티콘 사용 해주세요 \\n']\n",
            "실제 대답 : ['\\t 바코드 를 앞 에 기계 에 \\n']\n",
            "챗봇 대답 : ['네 잠시 만 기다려주세요 \\n']\n",
            "______\n",
            "질문 : \t['카페 는 몇 시 까지 하나요 \\n']\n",
            "실제 대답 : ['\\t 까지 합니다 \\n']\n",
            "챗봇 대답 : ['5분 정도 기다리세요 \\n']\n",
            "______\n",
            "질문 : \t['티 종류 도 아이스 가능한가요 \\n']\n",
            "실제 대답 : ['\\t 네 고객 님 티 종류 다 아이스 가능합니다 \\n']\n",
            "챗봇 대답 : ['네 4500원 입니다 \\n']\n",
            "______\n",
            "질문 : \t['머핀 은 뭐 가 제일 맛있나요 \\n']\n",
            "실제 대답 : ['\\t 블루베리 머핀 이 잘 나갑니다 \\n']\n",
            "챗봇 대답 : ['루이보스 는 루이보스 는 어떤 걸 로 하시겠어요 \\n']\n",
            "______\n",
            "질문 : \t['카페 와이파이 비밀번호 수 있을까요 \\n']\n",
            "실제 대답 : ['\\t 입니다 \\n']\n",
            "챗봇 대답 : ['매장 에서 드실 거 면 머그컵 에 드리도록 되어있습니다 \\n']\n",
            "______\n",
            "질문 : \t['아메리카노 두 잔 주문 할게요 \\n']\n",
            "실제 대답 : ['\\t 아메리카노 따뜻한 건가 요 \\n']\n",
            "챗봇 대답 : ['어떤 거 드릴 까요 \\n']\n",
            "______\n",
            "질문 : \t['카페모카 는 따뜻한 거 로 주세요 \\n']\n",
            "실제 대답 : ['\\t 카페모카 위 에 휘핑 올려 드릴 까요 \\n']\n",
            "챗봇 대답 : ['네 총 8000원 입니다 \\n']\n",
            "______\n",
            "질문 : \t['오늘 의 커피 는 커피 로 하나요 맛 이 \\n']\n",
            "실제 대답 : ['\\t 아 네 오늘 은 과테말라 커피 입니다 \\n']\n",
            "챗봇 대답 : ['네 \\n']\n",
            "______\n",
            "질문 : \t['여기 에서 으로 하나요 \\n']\n",
            "실제 대답 : ['\\t 을 하시면 됩니다 \\n']\n",
            "챗봇 대답 : ['5분 정도 기다리세요 \\n']\n",
            "______\n",
            "질문 : \t['현금 영수증 해주세요 \\n']\n",
            "실제 대답 : ['\\t 네 번호 찍어주세요 \\n']\n",
            "챗봇 대답 : ['네 번호 찍어주세요 \\n']\n",
            "______\n",
            "질문 : \t['스콘 두 포장 해주세요 \\n']\n",
            "실제 대답 : ['\\t 고객 님 데워 드릴 까요 \\n']\n",
            "챗봇 대답 : ['네 준비 해드릴게요 \\n']\n",
            "______\n",
            "질문 : \t['아이스 아메리카노 사이즈 업 해주세요 \\n']\n",
            "실제 대답 : ['\\t 아이스 아메리카노 4000원 입니다 \\n']\n",
            "챗봇 대답 : ['네 알겠습니다 \\n']\n",
            "______\n",
            "질문 : \t['와이파이 되나요 \\n']\n",
            "실제 대답 : ['\\t 와이파이 는 입니다 \\n']\n",
            "챗봇 대답 : ['네 죄송하지만 말차 케이크 는 품절 되었습니다 \\n']\n",
            "______\n",
            "질문 : \t['2 잔 주문 하면 얼마 죠 \\n']\n",
            "실제 대답 : ['\\t 7000원 입니다 \\n']\n",
            "챗봇 대답 : ['총 7000원 입니다 \\n']\n",
            "______\n",
            "질문 : \t['제 가 커피 를 음료 좀 추천 해주세요 \\n']\n",
            "실제 대답 : ['\\t 그럼 생 과 주스 가 가요 \\n']\n",
            "챗봇 대답 : ['네 알겠습니다 \\n']\n",
            "______\n",
            "질문 : \t['네 찍어주세요 \\n']\n",
            "실제 대답 : ['\\t 네 주문 딸기 스무디 와 쿠키 드릴게요 \\n']\n",
            "챗봇 대답 : ['네 드시고 가시나요 \\n']\n",
            "______\n",
            "질문 : \t['포인트 적립 해주세요 \\n']\n",
            "실제 대답 : ['\\t 네 번호 입력 부탁드립니다 \\n']\n",
            "챗봇 대답 : ['네 알겠습니다 \\n']\n",
            "______\n",
            "질문 : \t['단체 주문 가능한가요 \\n']\n",
            "실제 대답 : ['\\t 이 죠 \\n']\n",
            "챗봇 대답 : ['네 가능합니다 \\n']\n",
            "______\n",
            "질문 : \t['브레드 종류 는 뭐 가 있나요 \\n']\n",
            "실제 대답 : ['\\t 허니 브레드 와 치즈 브레드 가 있습니다 \\n']\n",
            "챗봇 대답 : ['티 생크림 가 잘 팔려요 \\n']\n",
            "______\n",
            "질문 : \t['그럼 자몽 차 한잔 주세요 \\n']\n",
            "실제 대답 : ['\\t 따뜻하게 드릴 까요 \\n']\n",
            "챗봇 대답 : ['네 사이즈 는 어떻게 드릴 까요 \\n']\n",
            "______\n",
            "질문 : \t['아이스 프라푸치노 한 잔이요 \\n']\n",
            "실제 대답 : ['\\t 아이스 프라푸치노 사이즈 는 어떻게 드릴 까요 \\n']\n",
            "챗봇 대답 : ['드시고 가세 요 \\n']\n",
            "______\n",
            "질문 : \t['밀크 티 에 혹시 우유 가 아니면 우유 이 \\n']\n",
            "실제 대답 : ['\\t 저희 는 우유 을 사용 하고 있어요 \\n']\n",
            "챗봇 대답 : ['네 사이즈 는 어떤 걸 드릴 까요 \\n']\n",
            "______\n",
            "질문 : \t['그럼 말차 라테 에 잘 케이크 하나 추천 해주시겠어요 \\n']\n",
            "실제 대답 : ['\\t 치즈 케이크 가 잘 같이 주문 하시는 고객 님 \\n']\n",
            "챗봇 대답 : ['네 포인트 나 할인 카드 있으세요 \\n']\n",
            "______\n",
            "질문 : \t['시럽 도 조금 넣어 주실 수 있나요 \\n']\n",
            "실제 대답 : ['\\t 시럽 은 500원 추가 됩니다 \\n']\n",
            "챗봇 대답 : ['고객 님 시럽 은 왼쪽 에 보시 면 있습니다 \\n']\n",
            "______\n",
            "질문 : \t['그럼 두유 로 바꿔서 카페라테 두 잔 주세요 \\n']\n",
            "실제 대답 : ['\\t 카페라테 사이즈 는 어떻게 드릴 까요 \\n']\n",
            "챗봇 대답 : ['네 진동 벨 드시고 가시나요 \\n']\n",
            "______\n",
            "질문 : \t['치즈 케이크 는 지금 없나요 \\n']\n",
            "실제 대답 : ['\\t 네 치즈케이크 는 지금 다 \\n']\n",
            "챗봇 대답 : ['네 유효 기간 내 라면 사용 가능하세요 \\n']\n",
            "______\n",
            "질문 : \t['카페라테 디카 페인 \\n']\n",
            "실제 대답 : ['\\t 네 \\n']\n",
            "챗봇 대답 : ['네 알겠습니다 \\n']\n",
            "______\n",
            "질문 : \t['하나 랑 아이스 아메리카노 하나 해서 주세요 \\n']\n",
            "실제 대답 : ['\\t 네 더 추가 하실 것 은 없으신 가요 \\n']\n",
            "챗봇 대답 : ['카페모카 원두 는 어떤 걸 로 할까 요 \\n']\n",
            "______\n",
            "질문 : \t['둘 다 톨 사이즈 로 주세요 \\n']\n",
            "실제 대답 : ['\\t 여기 서 드시고 요 \\n']\n",
            "챗봇 대답 : ['드시고 가시나요 \\n']\n",
            "______\n",
            "질문 : \t['화이트 아이스 도 있나요 \\n']\n",
            "실제 대답 : ['\\t 네 있습니다 \\n']\n",
            "챗봇 대답 : ['네 초코 머핀 이랑 꽃 있습니다 \\n']\n",
            "______\n",
            "질문 : \t['네 여기 텀블러 에 담아주세요 \\n']\n",
            "실제 대답 : ['\\t 고객 님 텀블러 하시면 500원 할인 됩니다 \\n']\n",
            "챗봇 대답 : ['네 엘지 님 포인트 총 5000 점 있으신 데 그중 4500 점 사용 도 와 드리겠습니다 \\n']\n",
            "______\n",
            "질문 : \t['적립 하고 있는데 시즌 메뉴 도 하나 만 적립 되나요 \\n']\n",
            "실제 대답 : ['\\t 네 \\n']\n",
            "챗봇 대답 : ['네 시즌 메뉴 는 있습니다 \\n']\n",
            "______\n",
            "질문 : \t['아아 주문 가능한가요 \\n']\n",
            "실제 대답 : ['\\t 네 가능합니다 \\n']\n",
            "챗봇 대답 : ['죄송합니다만 페퍼민트 는 방금 전 에 다 떨어졌어요 \\n']\n",
            "______\n",
            "질문 : \t['기프티콘 으로 결제 할게요 \\n']\n",
            "실제 대답 : ['\\t 네 그럼 쿠폰 저 보여주세요 \\n']\n",
            "챗봇 대답 : ['네 앞 에 카드 꽃 아 주시 면 됩니다 \\n']\n",
            "______\n",
            "질문 : \t['저 카푸치노 로 주문 할게요 \\n']\n",
            "실제 대답 : ['\\t 시럽 은 얼마나 드릴 까요 \\n']\n",
            "챗봇 대답 : ['플랫 화이트 사이즈 는 뭘 로 하시겠어요 \\n']\n",
            "______\n",
            "질문 : \t['아이스 아메리카노 1 잔 이랑 아이스 카페라테 1 잔 얼마 인가요 \\n']\n",
            "실제 대답 : ['\\t 아이스 아메리카노 1 잔 은 4000원 아이스 카페라테 1 잔 은 5000원 입니다 \\n']\n",
            "챗봇 대답 : ['500원 결제 해주시면 됩니다 \\n']\n",
            "______\n",
            "질문 : \t['테이크아웃 할게요 \\n']\n",
            "실제 대답 : ['\\t 지금 중 인데 케이크 주문 하시면 아메리카노 한잔 로 드려요 \\n']\n",
            "챗봇 대답 : ['사용 하시고 잔액 3천 원 인데 충전 드리고 요 \\n']\n",
            "______\n",
            "질문 : \t['밀크 티 종류 는 뭐 가 있어요 \\n']\n",
            "실제 대답 : ['\\t 루이보스 두 개 있습니다 \\n']\n",
            "챗봇 대답 : ['과일 생크림 케이크 가 잘 나가요 \\n']\n",
            "______\n",
            "질문 : \t['라테 아이스 도 되나요 \\n']\n",
            "실제 대답 : ['\\t 라테 는 돼요 \\n']\n",
            "챗봇 대답 : ['네 500원 할인 되세요 \\n']\n",
            "______\n",
            "질문 : \t['이 치즈케이크 도 한 조각 주세요 \\n']\n",
            "실제 대답 : ['\\t 여기 서 드실 건가 요 \\n']\n",
            "챗봇 대답 : ['네 총 9500원 결제 해드리겠습니다 \\n']\n",
            "______\n",
            "질문 : \t['아이스 로 변경 할 추가 요금 이 있나요 \\n']\n",
            "실제 대답 : ['\\t 아이스 음료 는 500원 이 더 \\n']\n",
            "챗봇 대답 : ['네 네 추가 하시면 추가 있습니다 \\n']\n",
            "______\n",
            "질문 : \t['카드 결제 할게요 \\n']\n",
            "실제 대답 : ['\\t 네 카드 받았습니다 \\n']\n",
            "챗봇 대답 : ['네 앞 에 카드 꽃 아 주시 면 됩니다 \\n']\n",
            "______\n",
            "질문 : \t['아이스 아메리카노 두 잔 주세요 \\n']\n",
            "실제 대답 : ['\\t 드시고 가시나요 \\n']\n",
            "챗봇 대답 : ['네 총 8000원 입니다 \\n']\n",
            "______\n",
            "질문 : \t['톨 사이즈 로 주문 할게요 \\n']\n",
            "실제 대답 : ['\\t 네 계산 도 와 드리겠습니다 \\n']\n",
            "챗봇 대답 : ['네 계산 해드리겠습니다 \\n']\n",
            "______\n",
            "질문 : \t['초코 머핀 도 하나 추가 해주세요 \\n']\n",
            "실제 대답 : ['\\t 초코 머핀 어떤 걸 로 드릴 까요 \\n']\n",
            "챗봇 대답 : ['네 네 결제 해드릴게요 \\n']\n",
            "______\n",
            "질문 : \t['얼마 인가요 \\n']\n",
            "실제 대답 : ['\\t 5천 원 입니다 \\n']\n",
            "챗봇 대답 : ['플랫 화이트 4500원 입니다 \\n']\n",
            "______\n",
            "질문 : \t['그러면 예 가체 주세요 \\n']\n",
            "실제 대답 : ['\\t 아메리카노 안 분 들 도 많이 선택 하세요 \\n']\n",
            "챗봇 대답 : ['다른 사항 은 드릴가요 \\n']\n",
            "______\n",
            "질문 : \t['여기 있습니다 \\n']\n",
            "실제 대답 : ['\\t 네 확인 되셨고 되면 진동 벨 거 예요 \\n']\n",
            "챗봇 대답 : ['네 고객 님 결제 완료 되었습니다 \\n']\n",
            "______\n",
            "질문 : \t['커피 캐리어 도 주실 수 있나요 \\n']\n",
            "실제 대답 : ['\\t 네 캐리어 에 담아 드릴게요 \\n']\n",
            "챗봇 대답 : ['네 있습니다 \\n']\n",
            "______\n",
            "질문 : \t['따뜻한 밀크 티 주세요 \\n']\n",
            "실제 대답 : ['\\t 네 \\n']\n",
            "챗봇 대답 : ['따뜻한 거 맞으세요 \\n']\n",
            "______\n",
            "질문 : \t['네 두 개 함께 \\n']\n",
            "실제 대답 : ['\\t 네 감사합니다 \\n']\n",
            "챗봇 대답 : ['네 알겠습니다 \\n']\n",
            "______\n",
            "질문 : \t['카페라테 한 잔 주세요 \\n']\n",
            "실제 대답 : ['\\t 카페라테 따뜻한 걸 로 드릴 까요 \\n']\n",
            "챗봇 대답 : ['네 카페라떼 컵 사이즈 는 뭘 로 드릴 까요 \\n']\n",
            "______\n",
            "질문 : \t['네 차가운 걸 로 주세요 \\n']\n",
            "실제 대답 : ['\\t 4500원 입니다 \\n']\n",
            "챗봇 대답 : ['드시고 가시나요 \\n']\n",
            "______\n",
            "질문 : \t['라이트 한 맛 의 아이스 아메리카노 요 \\n']\n",
            "실제 대답 : ['\\t 네 저희 메뉴 에요 \\n']\n",
            "챗봇 대답 : ['네 밀크 메뉴 는 있습니다 \\n']\n",
            "______\n",
            "질문 : \t['지금 되나요 \\n']\n",
            "실제 대답 : ['\\t 는 계절 메뉴 라 지금 은 판매 하지 않습니다 \\n']\n",
            "챗봇 대답 : ['네 유효 기간 내 라면 사용 가능하세요 \\n']\n",
            "______\n",
            "질문 : \t['사이즈 업 되나요 \\n']\n",
            "실제 대답 : ['\\t 네 \\n']\n",
            "챗봇 대답 : ['아니요 한 사이즈 로만 판매 하고 있습니다 \\n']\n",
            "______\n",
            "질문 : \t['아뇨 그냥 주세요 \\n']\n",
            "실제 대답 : ['\\t 할인 카드 도 안 하시나요 \\n']\n",
            "챗봇 대답 : ['4천원 입니다 \\n']\n",
            "______\n",
            "질문 : \t['치즈 브레드 에 생크림 을 수 는 없나요 \\n']\n",
            "실제 대답 : ['\\t 500원 의 추가 이 괜찮으세요 \\n']\n",
            "챗봇 대답 : ['네 500원 추가 하시면 추가 돼요 \\n']\n",
            "______\n",
            "질문 : \t['멤버십 할인 되나요 \\n']\n",
            "실제 대답 : ['\\t 네 멤버십 할인 하시면 10 할인 됩니다 \\n']\n",
            "챗봇 대답 : ['네 500원 할인 되세요 \\n']\n",
            "______\n",
            "질문 : \t['이 카드 로 결제 해주세요 \\n']\n",
            "실제 대답 : ['\\t 네 결제 도 와 드릴게요 \\n']\n",
            "챗봇 대답 : ['네 결제 에 와 됩니다 \\n']\n",
            "______\n",
            "질문 : \t['네 카드 여기 요 \\n']\n",
            "실제 대답 : ['\\t 적립 쿠폰 있으세요 \\n']\n",
            "챗봇 대답 : ['네 번호 적어주세요 \\n']\n",
            "______\n",
            "질문 : \t['티라미수 케이크 도 주세요 \\n']\n",
            "실제 대답 : ['\\t 적립 카드 있으신 가요 \\n']\n",
            "챗봇 대답 : ['네 아이스 필요하신 건 가요 \\n']\n",
            "______\n",
            "질문 : \t['아니요 테이크아웃 이 요 \\n']\n",
            "실제 대답 : ['\\t 네 포인트 적립 하시나요 \\n']\n",
            "챗봇 대답 : ['네 알겠습니다 \\n']\n",
            "______\n",
            "질문 : \t['아이스 아메리카노 하나 랑 녹차 라테 주세요 \\n']\n",
            "실제 대답 : ['\\t 녹차 라테 도 아이스 로 드릴 까요 \\n']\n",
            "챗봇 대답 : ['네 알겠습니다 \\n']\n",
            "______\n",
            "질문 : \t['매장 에서 먹고 갈 거 예요 \\n']\n",
            "실제 대답 : ['\\t 할인 카드 있으신 가요 \\n']\n",
            "챗봇 대답 : ['포크 는 몇 개 드릴 까요 \\n']\n",
            "______\n",
            "질문 : \t['아메리카노 한잔 주세요 \\n']\n",
            "실제 대답 : ['\\t 따뜻한 걸 로 드릴 까요 \\n']\n",
            "챗봇 대답 : ['드시고 가시나요 \\n']\n",
            "______\n",
            "질문 : \t['을 \\n']\n",
            "실제 대답 : ['\\t 이라 \\n']\n",
            "챗봇 대답 : ['네 드릴게요 잠시 요 \\n']\n",
            "______\n",
            "질문 : \t['바닐라 라테 도 하나 주세요 \\n']\n",
            "실제 대답 : ['\\t 바닐라 라테 따뜻한 거 요 \\n']\n",
            "챗봇 대답 : ['네 진동 벨 이 울리면 픽업 대로 와주세요 \\n']\n",
            "______\n",
            "질문 : \t['네 2 층 이 \\n']\n",
            "실제 대답 : ['\\t 네 2 층 에 자리 \\n']\n",
            "챗봇 대답 : ['더 필요한 건 없으세요 \\n']\n",
            "______\n",
            "질문 : \t['하시나요 \\n']\n",
            "실제 대답 : ['\\t 네 입니다 \\n']\n",
            "챗봇 대답 : ['쓴맛 은 많이 없고 산미 가 있고 다른 원두 들 보다 과일 향 이 나죠 \\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnQXNDWa0CjZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}