{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq 기본 구현",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/qGBm5x4tL4UZSztqyc3V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nureeee/DeepLearning/blob/main/Seq2Seq_%EA%B8%B0%EB%B3%B8_%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hO3nIVTyehy"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import LSTM"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPD7sTRsxK_2"
      },
      "source": [
        "sample_train = np.random.randn(1, 4, 5)  # N * L * I "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHFjRzA2yKRn",
        "outputId": "b2113706-52ab-49cf-d757-e0f51d348af3"
      },
      "source": [
        "last_hidden_state = LSTM(3, return_sequences=False, return_state=False)(sample_train)\n",
        "print(last_hidden_state)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[-0.06553418  0.05300762 -0.00716654]], shape=(1, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vDXQw57ybWI",
        "outputId": "7e76b270-b99a-4329-e6c4-75d9de243086"
      },
      "source": [
        "hidden_states, last_hidden_state, last_cell_state = LSTM(3, return_sequences=False, return_state=True)(sample_train)\n",
        "print('hidden_states : {}'.format(hidden_states))\n",
        "print('last_hidden_state : {}'.format(last_hidden_state))\n",
        "print('last_cell_state : {}'.format(last_cell_state))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hidden_states : [[ 0.13284302 -0.04286626  0.06677939]]\n",
            "last_hidden_state : [[ 0.13284302 -0.04286626  0.06677939]]\n",
            "last_cell_state : [[ 0.22402963 -0.11064757  0.15431193]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBbiH7RKzglK",
        "outputId": "e607251b-0359-4701-f590-117a450b0b88"
      },
      "source": [
        "hidden_states = LSTM(3, return_sequences=True, return_state=False)(sample_train)\n",
        "print('hidden_states : {} / shape : {}'.format(hidden_states, hidden_states.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hidden_states : [[[ 0.01692073 -0.02748816 -0.4977551 ]\n",
            "  [ 0.17771466  0.07629572 -0.31636286]\n",
            "  [-0.12964407  0.06626214 -0.07307925]\n",
            "  [-0.06907882 -0.13199747 -0.173571  ]]] / shape : (1, 4, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjG8Nm2N08A2",
        "outputId": "97e7e431-0738-4d06-8099-fdb6a51e08e8"
      },
      "source": [
        "! pip install konlpy"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 56.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX02eBdU3SMP"
      },
      "source": [
        "import random # 나중에 데이터 셔플링 할 예정\n",
        "import tensorflow as tf\n",
        "from konlpy.tag import Okt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPyAf5Vm3hUw"
      },
      "source": [
        "num_epochs = 200\n",
        "vocab_size = 2000"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25PFafz434km"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.emb = tf.keras.layers.Embedding(vocab_size, 64) # 임베딩 사이즈가 64??\n",
        "        self.lstm = tf.keras.layers.LSTM(512, return_sequences=False, return_state=True)\n",
        "        \n",
        "    def call(self, x, training=False):\n",
        "        x = self.emb(x)\n",
        "\n",
        "        _, h, c = self.lstm(x)\n",
        "        \n",
        "        return h, c\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VvOCpro4Hsl"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.emb = tf.keras.layers.Embedding(vocab_size, 64)\n",
        "        self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)\n",
        "        \n",
        "        self.dense = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x, h, c = inputs\n",
        "        x = self.emb(x)\n",
        "\n",
        "        # y_ : 해당 시퀀스이 hidden_state \n",
        "        y_, h, c = self.lstm(x, initial_state=[h, c])\n",
        "\n",
        "        y = self.dense(y_)\n",
        "        \n",
        "        return y, h, c"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD19oPhn4Mlb"
      },
      "source": [
        "class Seq2seq(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, sos, eos):\n",
        "        super(Seq2seq, self).__init__()\n",
        "        self.sos = sos\n",
        "        self.eos = eos\n",
        "\n",
        "        self.enc = Encoder()\n",
        "        self.dec = Decoder()\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        if training:\n",
        "            x, y = inputs # x는 인코더 y는 디코더\n",
        "            h, c = self.enc(x)\n",
        "            # y.shape : (N, 64, 64)\n",
        "            y, _, __ = self.dec((y, h, c))\n",
        "            return y\n",
        "        else:\n",
        "            x = inputs\n",
        "            h, c = self.enc(x)\n",
        "\n",
        "            y = tf.convert_to_tensor(self.sos)\n",
        "            y = tf.reshape(y, (1, 1))\n",
        "            \n",
        "            seq = tf.TensorArray(tf.int32, 64)\n",
        "\n",
        "            for idx in tf.range(64):\n",
        "                \n",
        "                y, h, c = self.dec([y, h, c])\n",
        "                y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32)\n",
        "\n",
        "                y = tf.reshape(y, (1, 1))\n",
        "\n",
        "                seq = seq.write(idx, y)\n",
        "\n",
        "                if y == self.eos:\n",
        "                    break\n",
        "                    \n",
        "            return tf.reshape(seq.stack(), (1, 64))\n",
        "                "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s4O1FrlNMeA"
      },
      "source": [
        "@tf.function\n",
        "def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy):\n",
        "    output_labels = labels[:, 1:]\n",
        "    shifted_labels = labels[:, :-1]\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model([inputs, shifted_labels], training=True)\n",
        "        loss = loss_object(output_labels, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(output_labels, predictions)\n",
        "    \n",
        "@tf.function\n",
        "def test_step(model, inputs):\n",
        "    return model(inputs, training=False)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2A81ruTkmax"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "dataset_file = \"chatbot_data.csv\"\n",
        "okt = Okt()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMChUczxkfbb"
      },
      "source": [
        "with open(dataset_file, 'r') as file:\n",
        "  lines = file.readlines()\n",
        "  seq = [\" \".join(okt.morphs(line)) for line in lines]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMRWB56NkUEU",
        "outputId": "401d5080-f826-404e-bae5-c4823347091c"
      },
      "source": [
        "questions = seq[::2]\n",
        "answers = ['\\t ' + lines for lines in seq[1::2]]\n",
        "print(questions[:3])\n",
        "print(answers[:3])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['아이스 아메리카노 하나요 \\n', '저 카푸치노 로 주문 할게요 \\n', '저 도장 다 모았는데 나중 에 써도 되나요 ? \\n']\n",
            "['\\t 테이크아웃 하실 건가 요 ? \\n', '\\t 시럽 은 얼마나 뿌려 드릴 까요 ? \\n', '\\t 네 다음 에 써도 됩니다 \\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOm6EnEvlQpP",
        "outputId": "daf437be-46aa-4daa-b3d5-ebf397937dc2"
      },
      "source": [
        "num_samples = len(questions)\n",
        "print(num_samples)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEiH3hfRlsJE",
        "outputId": "22b11599-9e7c-4f41-b0bb-37a766f608eb"
      },
      "source": [
        "term = list(range(num_samples))\n",
        "random.seed(0)\n",
        "random.shuffle(term)\n",
        "\n",
        "print(term)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[419, 459, 130, 431, 370, 26, 201, 56, 366, 108, 231, 326, 118, 153, 493, 311, 333, 24, 367, 17, 150, 295, 247, 44, 274, 285, 481, 420, 164, 100, 199, 196, 405, 62, 452, 436, 415, 58, 393, 64, 492, 98, 188, 433, 307, 127, 404, 210, 277, 256, 116, 27, 343, 84, 134, 177, 109, 417, 67, 317, 490, 129, 68, 270, 1, 390, 438, 28, 451, 139, 355, 160, 168, 413, 264, 354, 339, 291, 137, 174, 306, 105, 332, 15, 225, 359, 469, 279, 391, 187, 429, 251, 289, 377, 496, 262, 192, 103, 142, 22, 383, 444, 167, 184, 292, 14, 136, 203, 331, 165, 237, 321, 131, 369, 288, 375, 157, 146, 450, 235, 125, 206, 497, 437, 265, 43, 205, 69, 38, 297, 2, 449, 219, 53, 499, 380, 480, 117, 259, 477, 93, 176, 202, 52, 211, 10, 4, 221, 39, 351, 88, 283, 254, 65, 356, 115, 70, 5, 352, 101, 121, 208, 345, 476, 77, 462, 486, 30, 185, 25, 491, 263, 275, 180, 54, 194, 80, 186, 173, 213, 3, 238, 21, 362, 353, 474, 495, 12, 119, 236, 83, 126, 9, 410, 193, 473, 255, 458, 135, 434, 224, 232, 371, 379, 220, 357, 302, 281, 457, 387, 304, 216, 324, 284, 337, 233, 145, 246, 453, 318, 114, 407, 57, 253, 479, 85, 178, 49, 152, 428, 287, 346, 212, 106, 191, 341, 79, 402, 33, 463, 330, 475, 228, 242, 151, 91, 172, 73, 156, 90, 384, 446, 195, 315, 34, 143, 325, 340, 40, 234, 269, 78, 323, 198, 348, 159, 406, 209, 482, 6, 89, 445, 60, 471, 271, 175, 82, 81, 29, 395, 86, 240, 484, 483, 102, 314, 299, 494, 239, 257, 303, 92, 223, 179, 403, 36, 416, 374, 376, 365, 217, 440, 309, 329, 107, 245, 319, 382, 99, 11, 18, 335, 189, 389, 364, 61, 385, 310, 381, 399, 13, 460, 394, 273, 293, 23, 334, 74, 454, 441, 218, 397, 87, 190, 112, 344, 138, 8, 425, 418, 171, 322, 392, 249, 59, 378, 421, 42, 182, 328, 338, 426, 230, 140, 296, 214, 301, 372, 110, 120, 267, 141, 268, 400, 200, 349, 396, 358, 408, 19, 76, 66, 347, 45, 35, 243, 442, 336, 427, 16, 95, 96, 94, 148, 123, 294, 162, 498, 305, 472, 227, 147, 300, 398, 466, 448, 276, 487, 439, 63, 468, 149, 447, 154, 55, 250, 260, 163, 41, 46, 229, 278, 467, 72, 411, 122, 113, 290, 97, 32, 360, 166, 489, 124, 170, 423, 252, 313, 0, 320, 342, 488, 422, 363, 204, 430, 368, 47, 7, 280, 412, 31, 133, 266, 443, 226, 485, 282, 104, 327, 312, 161, 222, 181, 51, 286, 241, 169, 350, 435, 37, 373, 50, 158, 75, 461, 308, 414, 361, 272, 465, 128, 409, 316, 48, 386, 478, 144, 71, 258, 111, 464, 456, 298, 183, 244, 155, 424, 401, 470, 207, 248, 261, 132, 20, 215, 455, 388, 197, 432]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPVrFbQsmESA"
      },
      "source": [
        "train_q = []\n",
        "train_a = []\n",
        "\n",
        "test_q = []\n",
        "test_a = []"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NSchP1Cm5DQ"
      },
      "source": [
        "test_ratio = 0.2\n",
        "test_cnt = int(len(questions) * test_ratio)\n",
        "\n",
        "train_indices = term[test_cnt:]\n",
        "test_indices = term[:test_cnt]\n",
        "\n",
        "for idx in train_indices:\n",
        "    train_q.append(questions[idx])\n",
        "    train_a.append(answers[idx])\n",
        "\n",
        "for idx in test_indices:\n",
        "    test_q.append(questions[idx])\n",
        "    test_a.append(answers[idx])\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsvPAQUcnC-r",
        "outputId": "f5a1e951-9a3b-40c6-d5ba-e5e05d593692"
      },
      "source": [
        "test_q[:3], test_a[:3]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['사이 즈 업 해서 주세요 \\n',\n",
              "  '캐러멜 드리블 이랑 통 잡아 칩이요 \\n',\n",
              "  '시즌 메뉴 와 함께 구성 되어 있는 세트 메뉴 가 있나요 ? \\n'],\n",
              " ['\\t 네 결제 는 어떻게 도 와 드릴 까요 ? \\n',\n",
              "  '\\t 6700원 결제 도 와 드리겠습니다 \\n',\n",
              "  '\\t 네 치즈 케이크 와 시즌 메뉴 두 잔 으로 구성 된 세트 메뉴 있습니다 \\n'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-WDa9D3ogUE"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3xVAjJfpIBG",
        "outputId": "545b3f8a-829e-4095-df53-3ce821544fd2"
      },
      "source": [
        "tokenizer.fit_on_texts(train_q + train_a)\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\n': 1, '\\t': 2, '네': 3, '주세요': 4, '로': 5, '아메리카노': 6, '는': 7, '에': 8, '아이스': 9, '도': 10, '요': 11, '잔': 12, '이': 13, '한': 14, '드릴': 15, '까요': 16, '은': 17, '입니다': 18, '사이즈': 19, '가': 20, '있나요': 21, '결제': 22, '수': 23, '하나': 24, '있습니다': 25, '와': 26, '드시고': 27, '해주세요': 28, '할게요': 29, '으로': 30, '라테': 31, '추가': 32, '따뜻한': 33, '주문': 34, '사용': 35, '음료': 36, '되나요': 37, '여기': 38, '아니요': 39, '거': 40, '얼마': 41, '개': 42, '그럼': 43, '카드': 44, '랑': 45, '드리겠습니다': 46, '케이크': 47, '어떤': 48, '걸': 49, '포인트': 50, '가시나요': 51, '한잔': 52, '할인': 53, '적립': 54, '다': 55, '커피': 56, '더': 57, '인가요': 58, '쿠폰': 59, '가요': 60, '드릴게요': 61, '티': 62, '건': 63, '가능합니다': 64, '알겠습니다': 65, '에서': 66, '가능한가요': 67, '매장': 68, '를': 69, '진동': 70, '면': 71, '벨': 72, '안': 73, '번호': 74, '만': 75, '에요': 76, '메뉴': 77, '하나요': 78, '디카': 79, '페인': 80, '건가': 81, '샷': 82, '있어요': 83, '됩니다': 84, '테이크아웃': 85, '예요': 86, '스무디': 87, '게': 88, '카페라테': 89, '두': 90, '같이': 91, '자몽': 92, '하고': 93, '치즈케이크': 94, '제일': 95, '뭐': 96, '카페모카': 97, '기프티콘': 98, '세트': 99, '지금': 100, '종류': 101, '해서': 102, '업': 103, '먹고': 104, '휘핑크림': 105, '이랑': 106, '머핀': 107, '몇': 108, '어떻게': 109, '현금영수증': 110, '원': 111, '해드리겠습니다': 112, '하실': 113, '까지': 114, '초코': 115, '주스': 116, '화이트': 117, '프라푸치노': 118, '해': 119, '시럽': 120, '테이크': 121, '아웃': 122, '많이': 123, '베이글': 124, '2': 125, '다른': 126, '울리면': 127, '가능하세요': 128, '1': 129, '텀블러': 130, '저': 131, '갈': 132, '컵': 133, '주시': 134, '할': 135, '바닐라': 136, '크림': 137, '추천': 138, '없으신': 139, '4500원': 140, '해드릴게요': 141, '500원': 142, '총': 143, '포장': 144, '딸기': 145, '중': 146, '혹시': 147, '판매': 148, '밀크': 149, '영수증': 150, '과': 151, '둘': 152, '스콘': 153, '아': 154, '잘': 155, '찍어주세요': 156, '계산': 157, '루이보스': 158, '정도': 159, '맛': 160, '원두': 161, '필요한': 162, '올려': 163, '가실': 164, '준비': 165, '없으세요': 166, '어디': 167, '자리': 168, '얼마나': 169, '오늘': 170, '휘핑': 171, '얼음': 172, '그냥': 173, '화장실': 174, '담아': 175, '서': 176, '라지': 177, '있을까요': 178, '과일': 179, '카페인': 180, '것': 181, '가격': 182, '인데': 183, '있으세요': 184, '있으신': 185, '저희': 186, '치즈': 187, '플랫': 188, '죠': 189, '시즌': 190, '넣어': 191, '고객': 192, '님': 193, '알려': 194, '도장': 195, '되죠': 196, '그러면': 197, '티라미슈': 198, '변경': 199, '페이': 200, '데': 201, '카푸치노': 202, '주실': 203, '고': 204, '샌드위치': 205, '티라미수': 206, '현금': 207, '그렇게': 208, '쿠키': 209, '차갑게': 210, '아니오': 211, '일회용': 212, '따뜻하게': 213, '무료': 214, '이나': 215, '말차': 216, '나가요': 217, '영업': 218, '있는데': 219, '생크림': 220, '이드': 221, '을': 222, '유자차': 223, '조각': 224, '아뇨': 225, '유리잔': 226, '뭘': 227, '와주세요': 228, '10시': 229, '필요하신': 230, '해드릴까': 231, '가능하십니다': 232, '가세': 233, '말씀': 234, '완료': 235, '되었습니다': 236, '하시겠어요': 237, '선택': 238, '부탁드릴게요': 239, '때': 240, '핫초코': 241, '파나요': 242, '블루베리': 243, '담아주세요': 244, '건데': 245, '차가운': 246, '엔': 247, '뭔가': 248, '주': 249, '주시나요': 250, '넣어주세요': 251, '찾으러': 252, '될까': 253, '우유': 254, '부탁드려요': 255, '담아주실': 256, '스몰': 257, '10': 258, '가능할까': 259, '괜찮아요': 260, '키위': 261, '주차': 262, '조금': 263, '없나요': 264, '에스프레소': 265, '입력': 266, '카페라떼': 267, '페퍼민트': 268, '그리고': 269, '멤버십': 270, '앞': 271, '피지': 272, '제': 273, '녹차': 274, '바코드': 275, '와이파이': 276, '비밀번호': 277, '차': 278, '만들어': 279, '캐리어': 280, '겨울': 281, '시': 282, '있는': 283, '에는': 284, '쪽': 285, '잠시': 286, '10분': 287, '에서는': 288, '머그컵': 289, '않습니다': 290, '그럼요': 291, '하시면': 292, '하시는': 293, '가능해요': 294, '4000원': 295, '나': 296, '맞으세요': 297, '2000원': 298, '오시': 299, '레드': 300, '벨벳': 301, '언제': 302, '맛있는': 303, '모았는데': 304, '써도': 305, '하겠습니다': 306, '카카오': 307, '사이': 308, '즈': 309, '기다려야': 310, '의': 311, '개인': 312, '칩': 313, '적게': 314, '그': 315, '팥빙수': 316, '아이리쉬': 317, '있죠': 318, '갈게요': 319, '허니': 320, '브레드': 321, '직접': 322, '우려': 323, '시간': 324, '계절': 325, '부탁드립니다': 326, '모카': 327, '아아': 328, '만들어주세요': 329, '차는': 330, '캐러멜': 331, '들고': 332, '통신사': 333, '나갈': 334, '딸기스무디': 335, '생': 336, '모으면': 337, '마실': 338, '양': 339, '다음': 340, '번': 341, '이번': 342, '좀': 343, '하시나요': 344, '라떼': 345, '잔이요': 346, '걸려요': 347, '주시겠어요': 348, '어니언': 349, '접시': 350, '빵': 351, '민트': 352, '올려주세요': 353, '그건': 354, '쓸': 355, '하면': 356, '프라': 357, '페': 358, '맛있어요': 359, '걸릴까': 360, '충전': 361, '따로': 362, '있고': 363, '뜨거운': 364, '감귤': 365, '인절미': 366, '쏙쏙': 367, '붕어빵': 368, '고소한': 369, '품절': 370, '인': 371, '엘지': 372, '파이': 373, '전': 374, '대체': 375, '무엇': 376, '두유': 377, '없습니다': 378, '잘나가요': 379, '불가능합니다': 380, '괜찮으세요': 381, '300원': 382, '기다려주세요': 383, '내': 384, '상': 385, '드리고': 386, '이신': 387, '재료': 388, '바로': 389, '손님': 390, '하지': 391, '안됩니다': 392, '돼요': 393, '블랙': 394, '찍어': 395, '괜찮으신': 396, '픽업': 397, '해드려요': 398, '제공': 399, '오세요': 400, '받았습니다': 401, '플레인': 402, '기본': 403, '오후': 404, '따듯': 405, '안되고': 406, '톨': 407, '데워': 408, '5분': 409, '되셨습니다': 410, '9500원': 411, '카운터': 412, '문': 413, '없으시고요': 414, '보다': 415, '드려요': 416, '머그잔': 417, '해주시면': 418, '바꿔': 419, '5000원': 420, '점': 421, '부터': 422, '마감': 423, '앉아있다': 424, '가면': 425, '잘나가는': 426, '가져갈': 427, '기프트': 428, '콘': 429, '가능하나요': 430, '브런치': 431, '가져갈게요': 432, '빨대': 433, '아메리카': 434, '부탁': 435, '해요': 436, '나오나요': 437, '나중': 438, 'kt': 439, '더블': 440, '위': 441, '마시다가': 442, '들고나': 443, '그란': 444, '사이드': 445, '날씨': 446, '먹어요': 447, '받으려면': 448, '30': 449, '자바': 450, '빼고요': 451, '량': 452, '조절': 453, '걸리나요': 454, '십': 455, '시오': 456, '마카롱': 457, '남아있나요': 458, '하려고': 459, '대면': 460, '데워주실': 461, '줘요': 462, '남은': 463, '삼': 464, '성': 465, '에만': 466, '나오는': 467, '후': 468, '오면': 469, '받을': 470, '타서': 471, '연하게': 472, '만들어주실': 473, '모바일': 474, '함께': 475, '드리블': 476, '통': 477, '잡아': 478, '깔아주세요': 479, '전부': 480, '돼도': 481, '해주죠': 482, '안전히': 483, '있게': 484, '마시다': 485, '거니': 486, '없을까요': 487, '아이스커피': 488, '가능하죠': 489, '적은': 490, '숏': 491, '했어요': 492, '업은': 493, '삼성': 494, '덜': 495, '넣어주실': 496, '그린': 497, '20': 498, '말고': 499, '새로': 500, '적고': 501, '사람': 502, '당': 503, '씩': 504, '시켜야': 505, '아닌': 506, '들어간': 507, '휴대폰': 508, '마시려는데': 509, '명': 510, '창가': 511, '앉을': 512, '들어있지': 513, '않은': 514, '차감': 515, '으로는': 516, '없이': 517, '줄': 518, '스타벅스': 519, '당근': 520, '캐머': 521, '마일': 522, '물티슈': 523, '챙겨': 524, '큰': 525, '쿠앤크': 526, '치노': 527, '미지근한': 528, '물': 529, '그거': 530, '저번': 531, '와서': 532, '먹은': 533, '미숫가루': 534, '없네요': 535, '4': 536, '아직': 537, '런치': 538, '코': 539, '찡할': 540, '만큼': 541, '동시': 542, '가겠습니다': 543, '달': 544, '지': 545, '않나요': 546, '마키아토': 547, '들어가죠': 548, '먹을': 549, '맴버쉽': 550, '옮겨줄': 551, '콘센트': 552, '주시는데': 553, '들어가있나요': 554, '키': 555, '오스': 556, '크에서': 557, '되네요': 558, '나왔어요': 559, '해주신': 560, '3': 561, '어우러지는': 562, '업도': 563, '진하게': 564, '젤': 565, '좋아하는': 566, '리저브': 567, '처음': 568, '눌렀습니다': 569, '쓴가요': 570, '기존': 571, '통합': 572, '시킬': 573, '수도': 574, '레몬': 575, '분': 576, '들은': 577, '기계': 578, '인식': 579, '안되는데요': 580, '2만': 581, '밸': 582, '휘핑빼': 583, '우선': 584, '물이': 585, '치': 586, '오천': 587, '이요': 588, '먹을게요': 589, '감사합니다': 590, '하나같이': 591, '빅사': 592, '이즈': 593, '기프트카드': 594, '올라가는': 595, '작은': 596, '실내': 597, '가져왔는데': 598, '평소': 599, '주로': 600, '마시는데요': 601, '그런데': 602, '중간': 603, '대신': 604, '유로': 605, '빼주세요': 606, '빼주실': 607, '너무': 608, '달아서': 609, '싫은데': 610, '있으면': 611, '케익': 612, '교환': 613, '권': 614, '바꿀': 615, '로는': 616, '맛있나요': 617, '오픈': 618, '챙겨주세요': 619, '단것': 620, '별로': 621, '와는': 622, '해주시': 623, '카페': 624, '요금': 625, '바꿔서': 626, '올해': 627, '부터는': 628, '점포': 629, '1회': 630, '용을': 631, '바닥': 632, '청소': 633, '해야': 634, '죄송합니다': 635, '요새': 636, '라이트': 637, '5500원': 638, '바나나': 639, '파인애플': 640, '가지': 641, '계시다가': 642, '가져가세요': 643, '레귤러': 644, '14': 645, '나갑니다': 646, '팔려요': 647, '적용': 648, '9300원': 649, '규정': 650, '인기': 651, '기다려': 652, '단체': 653, '작아서': 654, '들어가요': 655, '6600원': 656, '팔렸어요': 657, '음식': 658, '이라': 659, '안되세요': 660, '되어': 661, '1만': 662, '1천': 663, '복도': 664, '나가시': 665, '보여요': 666, '가시는': 667, '괜찮으실까': 668, '8000원': 669, '드립니다': 670, '왼쪽': 671, '보시': 672, '7천원': 673, '만들고': 674, '진할': 675, '다시': 676, '해주시겠어요': 677, '드실': 678, '드리도록': 679, '되어있습니다': 680, '보여주세요': 681, '손잡이': 682, '투': 683, '들어가는데': 684, '대로': 685, '사항': 686, '곳': 687, '고르시면': 688, '보통': 689, '여자': 690, '분들': 691, '특정': 692, '선호': 693, '아니면': 694, '예': 695, '가체': 696, '프': 697, '냉동': 698, '제품': 699, '이고': 700, '다해': 701, '10900원': 702, '하프': 703, '되셔서': 704, '또': 705, '50': 706, '4천원': 707, '18000원': 708, '꽃': 709, '15000': 710, '1500원': 711, '어떠세요': 712, '500': 713, '기다리시면': 714, '안내': 715, '성분': 716, '들어가지': 717, '번만': 718, '테이블': 719, '하게': 720, '아예': 721, '빼는': 722, '최대한': 723, '하시고': 724, '잔액': 725, '3천': 726, '드렸고': 727, '이리': 728, '드릴가요': 729, '하세요': 730, '오전': 731, '이전': 732, '오셔서': 733, '한정': 734, '라': 735, '동일하게': 736, '저기': 737, '가져오시면': 738, '2시': 739, '가지러': 740, '곡물': 741, '가루': 742, '넣어서': 743, '고소하고': 744, '받으러': 745, '세': 746, '들어갑니다': 747, '호두': 748, '어울려요': 749, '적어주세요': 750, '층': 751, '나오면': 752, '로만': 753, '앉아': 754, '계시': 755, '가져다': 756, '기다리세요': 757, '오른쪽': 758, '벽쪽': 759, '건물': 760, '뒤': 761, '주차장': 762, '들어가있습니다': 763, '휴대': 764, '알려주시면': 765, '가능합니다만': 766, '천': 767, '올라가게': 768, '상큼': 769, '어울립니다': 770, '죄송합니다만': 771, '방금': 772, '떨어졌어요': 773, '유효': 774, '기간': 775, '라면': 776, '딱': 777, '맞게': 778, '오셨네요': 779, '원하시는': 780, '내리는': 781, '방법': 782, '대략': 783, '걸립니다': 784, '고맙습니다': 785, '쓴맛': 786, '없고': 787, '산미': 788, '들': 789, '향': 790, '나죠': 791, '합쳐': 792, '꽂아': 793, '6000원': 794, '이에요': 795, '밀려서': 796, '7분': 797, '걸릴': 798, '앞쪽': 799, '눌러주세요': 800, '불러': 801, '뜨거운건': 802, '할까': 803, '다모아': 804, '커피한잔': 805, '필요하세요': 806, '해드려도': 807, '괜찮을까요': 808, '떨어져서': 809, '어렵습니다': 810, '신': 811, '꺼': 812, '되는': 813, '8800원': 814, '해드렸습니다': 815, '정책': 816, '불가능하고': 817, '나가실': 818, '하시겠습니까': 819, '필요': 820, '다른거': 821, '필요한거': 822, '현재': 823, '법적': 824, '금지': 825, '종이': 826, '써져있습니다': 827, '한테': 828, '보여주시고': 829, '확인': 830, '버튼': 831, '누르면': 832, '요즘': 833, '없어요': 834, '다크': 835, '초콜릿': 836, '어우러진': 837, '달콤한': 838, '하셨나요': 839, '죄송하지만': 840, '과테말라': 841, '케냐': 842, '섞은': 843, '마이크로': 844, '블렌드': 845, '5천': 846, '필요하신가요': 847, '되세요': 848, '가시는거세요': 849, '7000원': 850, '포크': 851, '들어갔어요': 852, '4100원': 853, '2500원': 854, '당연하죠': 855, '팔려서요': 856, '나온': 857, '오곡': 858, '5000': 859, '그중': 860, '4500': 861, '가능한데': 862, '이상': 863, '9시': 864, '열어요': 865, '12시': 866, '까지라': 867, '드리면': 868, '되셨고': 869, '아주': 870, '단맛': 871, '아닌데요': 872, '라임': 873, '하단': 874, '적혀있습니다': 875, '합니다': 876, '되면': 877}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL3MPyK3pZRq"
      },
      "source": [
        "train_q_seq = tokenizer.texts_to_sequences(train_q)\n",
        "train_a_seq = tokenizer.texts_to_sequences(train_a)\n",
        "\n",
        "test_q_seq = tokenizer.texts_to_sequences(test_q)\n",
        "test_a_seq = tokenizer.texts_to_sequences(test_a)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuN7o_ftqeh4"
      },
      "source": [
        "X_train = pad_sequences(\n",
        "    train_q_seq,\n",
        "    value=0,\n",
        "    padding='pre',\n",
        "    maxlen=64\n",
        ")\n",
        "\n",
        "y_train = pad_sequences(\n",
        "    train_a_seq,\n",
        "    value=0,\n",
        "    padding='post',\n",
        "    maxlen=65\n",
        ")\n",
        "\n",
        "X_test = pad_sequences(\n",
        "    test_q_seq,\n",
        "    value=0,\n",
        "    padding='pre',\n",
        "    maxlen=64\n",
        ")\n",
        "\n",
        "y_test = pad_sequences(\n",
        "    test_a_seq,\n",
        "    value=0,\n",
        "    padding='post',\n",
        "    maxlen=65\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npakRDoosPiV",
        "outputId": "0f6194a1-4dba-4035-e9b1-4e811034db4b"
      },
      "source": [
        "X_train[0], y_train[0]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0, 85, 12, 30,  4,  1], dtype=int32),\n",
              " array([  2, 627, 628, 629,  73,  66, 630, 631,  35, 113,  23, 378,   1,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "       dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjDuxUlPsthp"
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1024).batch(32).prefetch(1024)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(1).prefetch(1024)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EdloKi4y1ga"
      },
      "source": [
        "model = Seq2seq(\n",
        "    sos=tokenizer.word_index[\"\\t\"],\n",
        "    eos=tokenizer.word_index[\"\\n\"]\n",
        ")\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNWD_zQCyvi6"
      },
      "source": [
        "EPOCHS = 200\n",
        "for epoch in range(EPOCHS):\n",
        "  for seqs, labels in train_ds:\n",
        "    train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy)\n",
        "  \n",
        "  print(\"Epoch : {}, Loss : {:.3f}, Accuracy : {:.3f}\".format(epoch + 1,\n",
        "                                                      train_loss.result(),\n",
        "                                                      train_accuracy.result() * 100))\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXPeBAwFzvU4"
      },
      "source": [
        "for test_seq, test_labels in test_ds:\n",
        "  prediction = test_step(model, test_seq)\n",
        "  \n",
        "  test_q = tokenizer.sequences_to_texts(test_seq.numpy())\n",
        "  test_a = tokenizer.sequences_to_texts(test_labels.numpy())\n",
        "  test_p = tokenizer.sequences_to_texts(prediction.numpy())\n",
        "\n",
        "  print(\"______\")\n",
        "  print(\"질문 : \\t{}\".format(test_q))\n",
        "  print(\"실제 대답 : {}\".format(test_a))\n",
        "  print(\"챗봇 대답 : {}\".format(test_p))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbQgkg6B190j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}